{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de paqueterías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a enumerar primeramente todas las paqueterías que necesitaremos a lo largo de nuestro código. En cada uno de los pasos, iremos especificando en dónde hacemos utilidad de cada una de las librerías importadas. Si hay alguna paquetería que tu sistema no tenga instalada, encontraremos el error marcado al importar las librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Paqueterías necesarias para la revisión de las versiones de nuestros programas ##################\n",
    "from platform import python_version\n",
    "import pkg_resources\n",
    "###########################################################################################################################\n",
    "\n",
    "######################### Librerías necesarias para trabajar con PyTorch ##################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "###########################################################################################################################\n",
    "\n",
    "####################### Librerías complementarias #########################################################################\n",
    "import time\n",
    "import read_data\n",
    "from collections import OrderedDict \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de nuestro código de Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a importar las funciones que usamos en el Notebook Set-Up Device, para poder trabajar, ya sea tanto en CPU, como en GPU, sin ningún problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)): #nos regresa True o False, si el data es del tipo especificado en la segunda entrada\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comienzo del trabajo formal de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿# 10/22/2019 water different molecules\n",
      "# Parameters to generate the G2 and G3 symmetry functions described by J Behler, JCP 134, 074106 (2011).\n",
      "\n",
      "# CentralAtom is the i-atom\n",
      "CentralAtom H\n",
      "\n",
      "# Number of symmetry functions 42\n",
      "Type G2 \n",
      "# j-atom eta Rs      Rc\n",
      "H    0.005  0.000  12.000\n",
      "O    0.005  0.000  12.000\n",
      "H    0.008  0.000  12.000\n",
      "O    0.008  0.000  12.000\n",
      "H    0.013  0.000  12.000\n",
      "O    0.013  0.000  12.000\n",
      "H    0.020  0.000  12.000\n",
      "O    0.020  0.000  12.000\n",
      "H    0.027  0.000  12.000\n",
      "O    0.027  0.000  12.000\n",
      "H    0.041  0.000  12.000\n",
      "O    0.041  0.000  12.000\n",
      "H    0.059  0.000  12.000\n",
      "O    0.059  0.000  12.000\n",
      "H    0.087  0.000  12.000\n",
      "O    0.087  0.000  12.000\n",
      "H    0.151  0.000  12.000\n",
      "O    0.151  0.000  12.000\n",
      "H    0.215  0.000  12.000\n",
      "O    0.215  0.000  12.000\n",
      "Type G4\n",
      "# j-k-atoms    eta     lambda   zeta    Rc\n",
      " H  H    0.000  -1.000   1.000  12.000\n",
      " H  O    0.000  -1.000   1.000  12.000\n",
      " O  O    0.000  -1.000   1.000  12.000\n",
      " H  H    0.000   1.000   1.000  12.000\n",
      " H  O    0.000   1.000   1.000  12.000\n",
      " O  O    0.000   1.000   1.000  12.000\n",
      " H  H    0.000  -1.000   2.000  12.000\n",
      " H  O    0.000  -1.000   2.000  12.000\n",
      " O  O    0.000  -1.000   2.000  12.000\n",
      " H  H    0.000   1.000   2.000  12.000\n",
      " H  O    0.000   1.000   2.000  12.000\n",
      " O  O    0.000   1.000   2.000  12.000\n",
      " H  H    0.000  -1.000   4.000  12.000\n",
      " H  O    0.000  -1.000   4.000  12.000\n",
      " O  O    0.000  -1.000   4.000  12.000\n",
      " H  H    0.000   1.000   4.000  12.000\n",
      " H  O    0.000   1.000   4.000  12.000\n",
      " O  O    0.000   1.000   4.000  12.000\n",
      " H  H    0.000  -1.000  16.000  12.000\n",
      " H  O    0.000  -1.000  16.000  12.000\n",
      " O  O    0.000  -1.000  16.000  12.000\n",
      " H  H    0.000   1.000  16.000  12.000\n",
      " H  O    0.000   1.000  16.000  12.000\n",
      " O  O    0.000   1.000  16.000  12.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_gen_xs = open(\"H-SymFunc.smf\")\n",
    "txt = to_gen_xs.read()\n",
    "print(txt)\n",
    "to_gen_xs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#```python\n",
    "# Ejemplo 3\n",
    "# This cell loads the data for the bo222-100k system\n",
    "\n",
    "system_name = 'bo222-100k-200k'             # Nombre del sistema\n",
    "data_path = \"./data/bo222-100k-200k-function.data\"            # Dirección de los datos para hacer el entrenamiento\n",
    "number_frames = 4000                        # Número de muestras\n",
    "number_atoms_per_frame = 222                # Número de átomos en cada muestra\n",
    "number_atoms_per_type = [98, 94, 30]     # Número de átomos de cada tipo\n",
    "number_atom_types = 3                        # Número de tipos de átomo           \n",
    "number_features_per_type = [78, 78, 78]     # Número de rasgos para cada tipo de átomo \n",
    "#```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum output: -64.90398406982422\n",
      "minimum output: -64.90711975097656\n",
      "maximum output: 1.8023798069533037\n",
      "minimum output: -0.4356204971488347\n",
      "reading and shifting and scaling data done. time: 241.70641326904297s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import read_data\n",
    "\n",
    "start_reading_data = time.time()\n",
    "\n",
    "data, energy_scale = read_data.read_data(data_path,number_frames,number_atoms_per_frame,number_atoms_per_type,number_features_per_type)\n",
    "\n",
    "end_reading_data = time.time()\n",
    "print(\"reading and shifting and scaling data done. time: {0}s\".format(end_reading_data-start_reading_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data es una <class 'list'> de tamaño 4000.\n",
      "Each element of the list is a frame object ( <frame.Frame object at 0x0000025A583639B0> ) \n",
      "that contains the features and the target of the corresponding xyz frame.\n"
     ]
    }
   ],
   "source": [
    "print(\"data es una {} de tamaño {}.\".format(type(data), len(data)))\n",
    "print(\"Each element of the list is a frame object ( {} ) \".format(data[0]))\n",
    "print(\"that contains the features and the target of the corresponding xyz frame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotación 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta esta parte del código, no se ha hecho cambio alguno con el framework de Keras. A partir de aquí, comenzamos a dividir data en data x y data y, y después, volveremos a partir los datas correspondientes, en tensores de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_x=[]\n",
    "for i in range(number_atom_types):\n",
    "    data_x.append(np.array([frame.get_input(i) for frame in data]))\n",
    "    \n",
    "data_y= np.array([frame.get_energy_of_frame_as_array() for frame in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x es una <class 'list'> de tamaño 3. \n",
      "\n",
      "Para el átomo tipo  1 , data_x[0] es un <class 'numpy.ndarray'> de forma (4000, 98, 78).\n",
      "Para el átomo tipo  2 , data_x[1] es un <class 'numpy.ndarray'> de forma (4000, 94, 78).\n",
      "Para el átomo tipo  3 , data_x[2] es un <class 'numpy.ndarray'> de forma (4000, 30, 78).\n"
     ]
    }
   ],
   "source": [
    "print(\"data_x es una {} de tamaño {}.\".format(type(data_x), len(data_x)), '\\n')\n",
    "\n",
    "for i in range(len(data_x)):\n",
    "    print(\"Para el átomo tipo \", i+1, \", data_x[{}] es un {} de forma {}.\".\n",
    "          format(i, type(data_x[i]), data_x[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotación 2\n",
    "\n",
    "Hasta esta parte, lo que tenemos es lo siguiente:\n",
    "- Tenemos un data_x, que es una lista de arreglos de numpy. Los arreglos de numpy se pueden convertir fácilmente a tensores de Pytorch, que es lo que haremos a continuación. Pero debemos partir la información en los diferentes átomos. Presentamos a continuación una figura que ilustra lo que tenemos en data_x, y lo que podemos hacer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./im_1_data_x.png\" width=520 height=520 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato que la organización del data_x no nos permite definir un tensor global data_x, podemos trabajar con diccionarios ordenados OrderedDict, para poder manejar nuestros datos de manera adecuada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_0_pt=torch.tensor(data_x[0])\n",
    "data_x_1_pt=torch.tensor(data_x[1])\n",
    "data_x_2_pt=torch.tensor(data_x[2])\n",
    "\n",
    "data_x_0_pt=to_device(data_x_0_pt, device)\n",
    "data_x_1_pt=to_device(data_x_1_pt, device)\n",
    "data_x_2_pt=to_device(data_x_2_pt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "data_x_pt=OrderedDict([\n",
    "    (0, data_x_0_pt),\n",
    "    (1, data_x_1_pt),\n",
    "    (2, data_x_2_pt)\n",
    "])\n",
    "\n",
    "data_y_pt=torch.tensor(data_y)\n",
    "data_y_pt=to_device(data_y_pt, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, vamos a poder entender cómo se manejan los datos de un framework a otro. En Keras, teníamos que poner el índice correspondiente, para conocer la información del atómo de tipo 0, 1 y 2. Por eso, hemos puesto las llaves de nuestro diccionario de esta manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 98, 78])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x_pt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 94, 78])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x_pt[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 30, 78])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x_pt[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./im_2_data_pt.png\" width=520 height=520 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenemos una imagen representativa de lo que se buscó hacer con el diccionario. Ya podemos manejar con mayor facilidad los datos. Y cada elemento del diccionario, ya es un tensor de Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra de las necesidades de tener que crear el diccionario de tensores, es que, a diferencia de Keras, no tenemos en Pytorch un layer de Input, que además tenga como variable desconocidas las dos primeras dimensiones del tensor, que es lo que hace `None` en Keras. Entonces, debemos conocer perfectamente las dimensiones de nuestros tensores, para poder alimentar a la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalmente, vamos a partir el problema en dos ciclos. La principal razón de hacer este procedimiento, es que para crear los batches, utilizaremos `torch.utils.data.DataLoader`, que tiene la restricción de que todos las muestras que tome, deben ser de las mismas dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=blue>\n",
    "    Histograma de un rasgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tipo_de_atomo=0\n",
    "\n",
    "rasgo=60\n",
    "\n",
    "plt.ylim(0,3)\n",
    "plt.hist(data_x_pt[tipo_de_atomo][:][:][rasgo].numpy(), bins='auto', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=blue>\n",
    "    Histograma de la energía total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_y_pt.numpy(), bins='auto', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=blue>\n",
    "    Construcción del modelo de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25a586166d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a contruir el modelo de aprendizaje, siguiendo como diagrama la siguiente imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./im_4_ciclos.png\" width=520 height=520 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos entonces tres entradas, provenientes de cada tipo de átomo. Pasarán individualmente a un ciclo individual (color azul), el cual tiene un subciclo (verde). Y teniendo el entrenamiento de los tres ciclos indoividuales, los juntaremos finalmente en el ciclo final (color rosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propiedad de broadcasting de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir la función especial de tangente hiperbólico "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a definir dicga función de activación, como una función cualquiera de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_plus_x(x):\n",
    "    layer=nn.Tanh()\n",
    "    return 1.7159 * layer((2.0*x)/3.0) + 0.01*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que sí trabaja nuestra función adecuadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1488, 0.7839])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba=torch.tensor([0.1488,0.7839])\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1711, 0.8310])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_plus_x(prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si somos observadores, en realidad no hubo necesidad de definir un tensor constante, y luego aplicar una función de multiplicación especial, para hacer nuestra función. Lo que pasó es que PyTorch tiene una propiedad, llamada **broadcasting**, la cual sigue una serie de reglas para multiplicar tensores bajo el símbolo *. Por eso, no tuvimos que lidiar mucho para definir esta función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, vamos a convertirla en una función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh_plus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return tanh_plus_x(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, probemos que todo selió bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "otro=tanh_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5129, 0.5389, 0.3555])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya=torch.rand(3)\n",
    "to_device(otro(ya),device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a comenzar con nuestro primer subciclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subciclo_layer(nn.Module):\n",
    "    def __init__(self,x_i,layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.x_i=x_i\n",
    "        self.dropout=dropout\n",
    "        for layer in self.layers[:-1]:\n",
    "            self.lineal=nn.Linear(x_i.shape[-1],layer)\n",
    "            torch.nn.init.uniform_(self.lineal.weight, a=0.0, b=1.0)\n",
    "            torch.nn.init.zeros_(self.lineal.bias)\n",
    "            self.tanh=tanh_plus()\n",
    "            if dropout:\n",
    "                self.dr=nn.Dropout(p=0.4)\n",
    "    def forward(self,x):\n",
    "        for layer in layers[:-1]:\n",
    "            out=self.lineal(x)\n",
    "            out=self.tanh(out)\n",
    "            if dropout:\n",
    "                self.dr(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a poner una prueba, para ver que nuestro modelo está funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[15,15,1]\n",
    "dropout=True\n",
    "x_pruebo=torch.rand(16,90,78)\n",
    "submodel=subciclo_layer(x_pruebo,layers, dropout)\n",
    "submodel=to_device(submodel, device) #vayamos acostumbrándonos a pasar todo lo que hagamos, al device de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 90, 15])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodel(x_pruebo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar algunas aclaraciones, que nos parecen pertinentes para comprender esta manera de generar un modelo en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primero, Pytorch también tiene su manera Secuencial de definir las capas que integrarán nuestro modelo; dicha forma de hacerlo es con `nn.Sequential(capas)` (NOTA: En Pytorch las capas (layers) se denominan módulos. Es importante recordarlo, porque puede ser que al consultar literatura, pueda existir confusión por simple definición.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nosotros no utilizaremos esta manera de programar, pues el modo secuencial se usa para capas que no necesitan más que parámetros fijos para definir los módulos (capas); es decir, no nos permite realizar modificaciones o integrar funciones de activación, como lo requerimos para este caso concreto del Notebook 15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora, seamos observadores, y comparemos el código de este subciclo, con el de la función de activación que definimos para la tangente hiperbólica. Hay un comando extra: `super().__init__()`. ¿Qué función cumple esta nueva línea de código? Nos permite declarar variables, que utilizaremos más adelante en la definición de nuestros layers como parámetros. Sin él, __init__ no puede leer estas variables dentro de los layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como estamos utilizando clases de `torch.nn`, hay que recordar que siempre necesitamos declararlos, con sus respectivos parámetros si son necesarios, previo a pasarles el tensor a calcular el modelo. Es por ende, que utilizamos __init__ para declarar las capas con los parámetros necesarios, y en __forward__ ya realizamos el procedimiento de calcular el tensor que buscamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso dentro de nuestro esquema de codificación de nuestro problema, es crear el ciclo individual, que deberemos aplicar a nuestros tipos de átomos, para después en un paso posterior, sumar el resultado de cada uno de los modelos individuales. Es aquí donde integraremos nuestro subciclo. recordando que **ya es una clase de nn**. Por lo tanto, no nos olvidemos de declararlo primero en __init__, y luego pasarle los datos necesarios a __forward__ para completar el cálculo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My_Layer en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder programar este módulo (layer) en Pytorch, primero, vamos a entender qué es lo que hace la función, comenzando priemro analizando la sintaxis que viene en Keras\n",
    "\n",
    "- Primero, lo que en Layer llama `call`, nosotros lo traducimos en Pytorch como `forward`. \n",
    "\n",
    "- Luego, vemos que aplica una suma. Como tenemos tres dimensiones, podemos especificar dos ejes, sobre los cuales se realizará la suma. Al seleccionar como ejes `axis=(1,2)`, lo que sucede es que vamos a sumar cada elemento de las rebanadas que componen nuestro tensor en tres dimensiones. Anexamos un dibujo ilustrativo de lo que hace `K.sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./im_3_ksum.png\" width=920 height=920 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalmente, como tenemos un tensor de dimensiones 1x3 en el caso del ejemplo de la figura, le aplicamos un reshape, para que quede de 3x1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo entendido lo que realiza el código de My_Layer, vamos a traducirlo a PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_layer(nn.Module):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(My_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.reshape(torch.sum(x, axis=(1,2)),(-1,self.output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que funcione adecuadamente My_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "apli=My_layer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1573],\n",
       "        [3.5457],\n",
       "        [2.2692],\n",
       "        [4.3796],\n",
       "        [4.1776],\n",
       "        [4.7145],\n",
       "        [2.5673],\n",
       "        [3.5006],\n",
       "        [2.2882],\n",
       "        [4.4589],\n",
       "        [2.3788],\n",
       "        [4.7249],\n",
       "        [6.3124],\n",
       "        [3.5455],\n",
       "        [5.0423],\n",
       "        [5.0376]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueb1=torch.rand(16,2,4)\n",
    "to_device(apli(prueb1),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ciclo_individual(nn.Module):\n",
    "    def __init__(self,x_i,layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.x_i=x_i\n",
    "        self.dropout=dropout\n",
    "        self.ML=My_layer(1)\n",
    "        \n",
    "        self.subciclo=subciclo_layer(x_i,layers,dropout)\n",
    "        self.lineal=nn.Linear(layers[-2],layers[-1])\n",
    "        torch.nn.init.uniform_(self.lineal.weight, a=0.0, b=1.0)\n",
    "        torch.nn.init.zeros_(self.lineal.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.subciclo(x)\n",
    "        out=self.lineal(out)\n",
    "        out=self.ML(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[15,15,1]\n",
    "dropout=True\n",
    "x_prueba=torch.rand(16,90,78)\n",
    "submodel=ciclo_individual(x_prueba,layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1253.6292],\n",
       "        [1252.1151],\n",
       "        [1253.4590],\n",
       "        [1254.0170],\n",
       "        [1255.8977],\n",
       "        [1254.7489],\n",
       "        [1252.4786],\n",
       "        [1252.9811],\n",
       "        [1254.1718],\n",
       "        [1253.6630],\n",
       "        [1255.4050],\n",
       "        [1253.7975],\n",
       "        [1255.2604],\n",
       "        [1254.2384],\n",
       "        [1254.1342],\n",
       "        [1253.4880]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodel(x_prueba)\n",
    "submodel=to_device(submodel,device)\n",
    "salida=submodel(x_prueba)\n",
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una capa que no viene explícitamente en Pytorch, pero que en Keras lo agregamos con `from keras.layers.merge import Add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merge_add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, out1, out2, out3):\n",
    "        zeros=torch.zeros(out1.shape)\n",
    "        lista_ten=[out1,out2,out3]\n",
    "        for out in lista_ten:\n",
    "            zeros += out\n",
    "        return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, crearemos el ciclo final para terminar nuestra arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ciclo_final(nn.Module):\n",
    "     def __init__(self, x_i, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.indivi= ciclo_individual(x_i, layers, dropout)\n",
    "        self.add=Merge_add()\n",
    "     \n",
    "     def forward(self, x1,x2,x3):\n",
    "        out1=self.indivi(x1)\n",
    "        out2=self.indivi(x2)\n",
    "        out3=self.indivi(x3)\n",
    "        out=self.add(out1,out2,out3)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[15,15,1]\n",
    "dropout=True\n",
    "x1=torch.rand(16,90,78)\n",
    "x2=torch.rand(16,80,78)\n",
    "x3=torch.rand(16,80,78)\n",
    "\n",
    "modelo=ciclo_final(x1,layers,dropout)\n",
    "modelo=to_device(modelo, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo(x1,x2,x3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ciclo_final(\n",
       "  (indivi): ciclo_individual(\n",
       "    (ML): My_layer()\n",
       "    (subciclo): subciclo_layer(\n",
       "      (lineal): Linear(in_features=78, out_features=15, bias=True)\n",
       "      (tanh): tanh_plus()\n",
       "      (dr): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (lineal): Linear(in_features=15, out_features=1, bias=True)\n",
       "  )\n",
       "  (add): Merge_add()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1         [-1, 4000, 98, 15]           1,185\n",
      "         tanh_plus-2         [-1, 4000, 98, 15]               0\n",
      "           Dropout-3         [-1, 4000, 98, 15]               0\n",
      "            Linear-4         [-1, 4000, 98, 15]           1,185\n",
      "         tanh_plus-5         [-1, 4000, 98, 15]               0\n",
      "           Dropout-6         [-1, 4000, 98, 15]               0\n",
      "    subciclo_layer-7         [-1, 4000, 98, 15]               0\n",
      "            Linear-8          [-1, 4000, 98, 1]              16\n",
      "          My_layer-9                    [-1, 1]               0\n",
      " ciclo_individual-10                    [-1, 1]               0\n",
      "           Linear-11         [-1, 4000, 94, 15]           1,185\n",
      "        tanh_plus-12         [-1, 4000, 94, 15]               0\n",
      "          Dropout-13         [-1, 4000, 94, 15]               0\n",
      "           Linear-14         [-1, 4000, 94, 15]           1,185\n",
      "        tanh_plus-15         [-1, 4000, 94, 15]               0\n",
      "          Dropout-16         [-1, 4000, 94, 15]               0\n",
      "   subciclo_layer-17         [-1, 4000, 94, 15]               0\n",
      "           Linear-18          [-1, 4000, 94, 1]              16\n",
      "         My_layer-19                    [-1, 1]               0\n",
      " ciclo_individual-20                    [-1, 1]               0\n",
      "           Linear-21         [-1, 4000, 30, 15]           1,185\n",
      "        tanh_plus-22         [-1, 4000, 30, 15]               0\n",
      "          Dropout-23         [-1, 4000, 30, 15]               0\n",
      "           Linear-24         [-1, 4000, 30, 15]           1,185\n",
      "        tanh_plus-25         [-1, 4000, 30, 15]               0\n",
      "          Dropout-26         [-1, 4000, 30, 15]               0\n",
      "   subciclo_layer-27         [-1, 4000, 30, 15]               0\n",
      "           Linear-28          [-1, 4000, 30, 1]              16\n",
      "         My_layer-29                    [-1, 1]               0\n",
      " ciclo_individual-30                    [-1, 1]               0\n",
      "        Merge_add-31                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 7,158\n",
      "Trainable params: 7,158\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 24.00\n",
      "Forward/backward pass size (MB): 718.14\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 742.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "batch_size=16\n",
    "summary(modelo, [data_x_pt[0].shape,data_x_pt[1].shape,data_x_pt[2].shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de los Dataset para poder tener los Mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0_train=[]\n",
    "dataset_0_val=[]\n",
    "dataset_1_train=[]\n",
    "dataset_1_val=[]\n",
    "dataset_2_train=[]\n",
    "dataset_2_val=[]\n",
    "\n",
    "val_rate=0.1\n",
    "val_integer=int((1-val_rate)*(data_x_pt[0].shape[0]))\n",
    "val_integer\n",
    "for i in range(data_x_pt[0].shape[0]):\n",
    "    if i < val_integer:\n",
    "        dataset_0_train.append((data_x_pt[0][i],data_y_pt[i]))\n",
    "        dataset_1_train.append((data_x_pt[1][i],data_y_pt[i]))\n",
    "        dataset_2_train.append((data_x_pt[2][i],data_y_pt[i]))\n",
    "    else:\n",
    "        dataset_0_val.append((data_x_pt[0][i],data_y_pt[i]))\n",
    "        dataset_1_val.append((data_x_pt[1][i],data_y_pt[i]))\n",
    "        dataset_2_val.append((data_x_pt[2][i],data_y_pt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_0=torch.utils.data.DataLoader(dataset_0_train, batch_size=16, shuffle=True)\n",
    "train_loader_1=torch.utils.data.DataLoader(dataset_1_train, batch_size=16, shuffle=True)\n",
    "train_loader_2=torch.utils.data.DataLoader(dataset_2_train, batch_size=16, shuffle=True)\n",
    "val_loader_0=torch.utils.data.DataLoader(dataset_0_val, batch_size=16, shuffle=True)\n",
    "val_loader_1=torch.utils.data.DataLoader(dataset_1_val, batch_size=16, shuffle=True)\n",
    "val_loader_2=torch.utils.data.DataLoader(dataset_2_val, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar nuestro `DeviceDataLoader` en esta parte del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_0=DeviceDataLoader(train_loader_0, device)\n",
    "train_loader_1=DeviceDataLoader(train_loader_1, device)\n",
    "train_loader_2=DeviceDataLoader(train_loader_2, device)\n",
    "\n",
    "val_loader_0=DeviceDataLoader(val_loader_0, device)\n",
    "val_loader_1=DeviceDataLoader(val_loader_1, device)\n",
    "val_loader_2=DeviceDataLoader(val_loader_2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La razón de haber hecho este paso, es que DataLoader no acepta tensores de diferentes tamaños. Por lo que tuvimos que crearlos nosotros mismos. Igualmente, recordemos que la desventaja que tenemos en PyTorch, es que debemos conocer las dimensiones, por lo que no pudimos usar `None` en nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=ciclo_final(data_x_pt[0],layers, dropout)\n",
    "modelo=to_device(modelo, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def training_loop(num_epochs, modelo):\n",
    "    criterion = nn. MSELoss()\n",
    "    learning_rate = 0.005\n",
    "    optimizer = optim.Adagrad(modelo.parameters(), lr=learning_rate,lr_decay=0, weight_decay=0, initial_accumulator_value=1.0, eps=1e-08)\n",
    "    for i in range(1, num_epochs+1):\n",
    "        running_loss=0\n",
    "        for (x0, y0), (x1, y1),(x2, y2) in zip(train_loader_0, train_loader_1, train_loader_2):\n",
    "            outputs=modelo(x1,x2,x3)\n",
    "            loss = criterion(outputs.float(), y2.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(\"Epoch: {}, Training loss: {}\".format(i,running_loss))\n",
    "\n",
    "                    \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "\n",
    "En el cálculo del gasto (loss), tuvimos que agregar `.float()` a ambos argumentos de la función, ya que de lo contrario, el programa marca un error, pues recibirá argumentos con distintos `dtype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 286828134.3125\n",
      "Epoch: 2, Training loss: 174682611.5\n",
      "Epoch: 3, Training loss: 124959381.40625\n",
      "Epoch: 4, Training loss: 94005432.21875\n",
      "Epoch: 5, Training loss: 72653878.34375\n",
      "Epoch: 6, Training loss: 57062901.0\n",
      "Epoch: 7, Training loss: 45303212.359375\n",
      "Epoch: 8, Training loss: 36217947.59375\n",
      "Epoch: 9, Training loss: 29103888.4921875\n",
      "Epoch: 10, Training loss: 23453638.046875\n",
      "Epoch: 11, Training loss: 18929713.53125\n",
      "Epoch: 12, Training loss: 15288665.5703125\n",
      "Epoch: 13, Training loss: 12354773.3984375\n",
      "Epoch: 14, Training loss: 9974137.322265625\n",
      "Epoch: 15, Training loss: 8067842.73046875\n",
      "Epoch: 16, Training loss: 6527387.953125\n",
      "Epoch: 17, Training loss: 5291092.3984375\n",
      "Epoch: 18, Training loss: 4296340.36328125\n",
      "Epoch: 19, Training loss: 3492490.734375\n",
      "Epoch: 20, Training loss: 2848593.9697265625\n",
      "Epoch: 21, Training loss: 2324789.1826171875\n",
      "Epoch: 22, Training loss: 1896337.9809570312\n",
      "Epoch: 23, Training loss: 1553429.2138671875\n",
      "Epoch: 24, Training loss: 1273451.5205078125\n",
      "Epoch: 25, Training loss: 1044174.3178710938\n",
      "Epoch: 26, Training loss: 862226.4714355469\n",
      "Epoch: 27, Training loss: 708407.1005859375\n",
      "Epoch: 28, Training loss: 588057.9382324219\n",
      "Epoch: 29, Training loss: 490624.4353027344\n",
      "Epoch: 30, Training loss: 408820.13555908203\n",
      "Epoch: 31, Training loss: 344572.48400878906\n",
      "Epoch: 32, Training loss: 290042.1043701172\n",
      "Epoch: 33, Training loss: 243485.70727539062\n",
      "Epoch: 34, Training loss: 213446.9887084961\n",
      "Epoch: 35, Training loss: 182344.62481689453\n",
      "Epoch: 36, Training loss: 163030.45651245117\n",
      "Epoch: 37, Training loss: 144018.07754516602\n",
      "Epoch: 38, Training loss: 128328.43849182129\n",
      "Epoch: 39, Training loss: 116106.87878417969\n",
      "Epoch: 40, Training loss: 108174.47067260742\n",
      "Epoch: 41, Training loss: 96658.11241149902\n",
      "Epoch: 42, Training loss: 91372.19813537598\n",
      "Epoch: 43, Training loss: 84090.05039215088\n",
      "Epoch: 44, Training loss: 82867.10498046875\n",
      "Epoch: 45, Training loss: 79529.73316192627\n",
      "Epoch: 46, Training loss: 77322.21310424805\n",
      "Epoch: 47, Training loss: 72846.66188812256\n",
      "Epoch: 48, Training loss: 72247.56480407715\n",
      "Epoch: 49, Training loss: 69766.47533416748\n",
      "Epoch: 50, Training loss: 67223.45383453369\n",
      "Epoch: 51, Training loss: 66483.75583648682\n",
      "Epoch: 52, Training loss: 67072.02891540527\n",
      "Epoch: 53, Training loss: 66406.15648651123\n",
      "Epoch: 54, Training loss: 66085.23029327393\n",
      "Epoch: 55, Training loss: 64164.37089538574\n",
      "Epoch: 56, Training loss: 65042.82503128052\n",
      "Epoch: 57, Training loss: 61679.22325134277\n",
      "Epoch: 58, Training loss: 65436.10414123535\n",
      "Epoch: 59, Training loss: 63473.17670440674\n",
      "Epoch: 60, Training loss: 62097.69613647461\n",
      "Epoch: 61, Training loss: 60128.67184448242\n",
      "Epoch: 62, Training loss: 61367.6583404541\n",
      "Epoch: 63, Training loss: 60847.837631225586\n",
      "Epoch: 64, Training loss: 60457.67236328125\n",
      "Epoch: 65, Training loss: 62049.80416107178\n",
      "Epoch: 66, Training loss: 59946.79956817627\n",
      "Epoch: 67, Training loss: 58709.07489013672\n",
      "Epoch: 68, Training loss: 59714.74391937256\n",
      "Epoch: 69, Training loss: 59930.01629638672\n",
      "Epoch: 70, Training loss: 58749.48114776611\n",
      "Epoch: 71, Training loss: 58881.14796447754\n",
      "Epoch: 72, Training loss: 56889.20567703247\n",
      "Epoch: 73, Training loss: 58029.945739746094\n",
      "Epoch: 74, Training loss: 58774.3411026001\n",
      "Epoch: 75, Training loss: 57922.12759399414\n",
      "Epoch: 76, Training loss: 54447.2236366272\n",
      "Epoch: 77, Training loss: 58034.494705200195\n",
      "Epoch: 78, Training loss: 54209.252212524414\n",
      "Epoch: 79, Training loss: 56482.22883605957\n",
      "Epoch: 80, Training loss: 55529.963134765625\n",
      "Epoch: 81, Training loss: 55910.53672027588\n",
      "Epoch: 82, Training loss: 55223.72314453125\n",
      "Epoch: 83, Training loss: 56013.59585571289\n",
      "Epoch: 84, Training loss: 54079.85541534424\n",
      "Epoch: 85, Training loss: 53413.229415893555\n",
      "Epoch: 86, Training loss: 52420.99024963379\n",
      "Epoch: 87, Training loss: 54983.50226211548\n",
      "Epoch: 88, Training loss: 53627.989181518555\n",
      "Epoch: 89, Training loss: 52597.57531738281\n",
      "Epoch: 90, Training loss: 54691.61656951904\n",
      "Epoch: 91, Training loss: 53749.702056884766\n",
      "Epoch: 92, Training loss: 53738.56474685669\n",
      "Epoch: 93, Training loss: 54151.16304779053\n",
      "Epoch: 94, Training loss: 51902.05122375488\n",
      "Epoch: 95, Training loss: 52240.2180480957\n",
      "Epoch: 96, Training loss: 51499.30881500244\n",
      "Epoch: 97, Training loss: 51251.7592086792\n",
      "Epoch: 98, Training loss: 52525.47255706787\n",
      "Epoch: 99, Training loss: 51670.72424316406\n",
      "Epoch: 100, Training loss: 49014.294216156006\n"
     ]
    }
   ],
   "source": [
    "training_loop(100,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loss(modelo, num_epochs):\n",
    "    modelo.eval()\n",
    "    criterion = nn. MSELoss()\n",
    "    learning_rate = 0.005\n",
    "    optimizer = optim.Adagrad(modelo.parameters(), lr=learning_rate,lr_decay=0, weight_decay=0, initial_accumulator_value=1.0, eps=1e-08)\n",
    "    for i in range(1, num_epochs+1):\n",
    "        running_loss=0\n",
    "        for (x0, y0), (x1, y1),(x2, y2) in zip(val_loader_0, val_loader_1, val_loader_2):\n",
    "            with torch.no_grad():\n",
    "                outputs=modelo(x1,x2,x3)\n",
    "                loss = criterion(outputs.float(), y0.float())\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                #loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "        print(\"Epoch: {}, Training loss: {}\".format(i,running_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 6625.365493774414\n",
      "Epoch: 2, Training loss: 6399.7132568359375\n",
      "Epoch: 3, Training loss: 6578.537979125977\n",
      "Epoch: 4, Training loss: 6548.339157104492\n",
      "Epoch: 5, Training loss: 6398.20573425293\n",
      "Epoch: 6, Training loss: 6450.496643066406\n",
      "Epoch: 7, Training loss: 6526.926025390625\n",
      "Epoch: 8, Training loss: 6446.53971862793\n",
      "Epoch: 9, Training loss: 6443.266265869141\n",
      "Epoch: 10, Training loss: 6467.658721923828\n",
      "Epoch: 11, Training loss: 6430.116592407227\n",
      "Epoch: 12, Training loss: 6382.395858764648\n",
      "Epoch: 13, Training loss: 6332.777725219727\n",
      "Epoch: 14, Training loss: 6419.305587768555\n",
      "Epoch: 15, Training loss: 6530.495307922363\n",
      "Epoch: 16, Training loss: 6435.654083251953\n",
      "Epoch: 17, Training loss: 6584.871246337891\n",
      "Epoch: 18, Training loss: 6381.272689819336\n",
      "Epoch: 19, Training loss: 6469.285293579102\n",
      "Epoch: 20, Training loss: 6332.043960571289\n",
      "Epoch: 21, Training loss: 6484.612686157227\n",
      "Epoch: 22, Training loss: 6488.5970458984375\n",
      "Epoch: 23, Training loss: 6492.302169799805\n",
      "Epoch: 24, Training loss: 6387.183364868164\n",
      "Epoch: 25, Training loss: 6590.872085571289\n",
      "Epoch: 26, Training loss: 6519.974746704102\n",
      "Epoch: 27, Training loss: 6361.083160400391\n",
      "Epoch: 28, Training loss: 6340.061096191406\n",
      "Epoch: 29, Training loss: 6598.295196533203\n",
      "Epoch: 30, Training loss: 6455.040451049805\n",
      "Epoch: 31, Training loss: 6437.839385986328\n",
      "Epoch: 32, Training loss: 6465.297409057617\n",
      "Epoch: 33, Training loss: 6720.800033569336\n",
      "Epoch: 34, Training loss: 6471.627090454102\n",
      "Epoch: 35, Training loss: 6787.404037475586\n",
      "Epoch: 36, Training loss: 6454.569717407227\n",
      "Epoch: 37, Training loss: 6377.769973754883\n",
      "Epoch: 38, Training loss: 6354.924041748047\n",
      "Epoch: 39, Training loss: 6336.09326171875\n",
      "Epoch: 40, Training loss: 6501.58528137207\n",
      "Epoch: 41, Training loss: 6366.453414916992\n",
      "Epoch: 42, Training loss: 6397.438858032227\n",
      "Epoch: 43, Training loss: 6449.812942504883\n",
      "Epoch: 44, Training loss: 6689.3984375\n",
      "Epoch: 45, Training loss: 6607.479110717773\n",
      "Epoch: 46, Training loss: 6680.485549926758\n",
      "Epoch: 47, Training loss: 6393.813781738281\n",
      "Epoch: 48, Training loss: 6402.459930419922\n",
      "Epoch: 49, Training loss: 6368.346374511719\n",
      "Epoch: 50, Training loss: 6254.479827880859\n",
      "Epoch: 51, Training loss: 6338.469512939453\n",
      "Epoch: 52, Training loss: 6480.094566345215\n",
      "Epoch: 53, Training loss: 6533.655548095703\n",
      "Epoch: 54, Training loss: 6502.866516113281\n",
      "Epoch: 55, Training loss: 6268.500137329102\n",
      "Epoch: 56, Training loss: 6598.194046020508\n",
      "Epoch: 57, Training loss: 6535.737762451172\n",
      "Epoch: 58, Training loss: 6518.417816162109\n",
      "Epoch: 59, Training loss: 6511.028434753418\n",
      "Epoch: 60, Training loss: 6360.313415527344\n",
      "Epoch: 61, Training loss: 6413.560073852539\n",
      "Epoch: 62, Training loss: 6352.520248413086\n",
      "Epoch: 63, Training loss: 6523.393524169922\n",
      "Epoch: 64, Training loss: 6308.303009033203\n",
      "Epoch: 65, Training loss: 6437.535598754883\n",
      "Epoch: 66, Training loss: 6572.778839111328\n",
      "Epoch: 67, Training loss: 6557.931076049805\n",
      "Epoch: 68, Training loss: 6664.628273010254\n",
      "Epoch: 69, Training loss: 6464.171478271484\n",
      "Epoch: 70, Training loss: 6380.418319702148\n",
      "Epoch: 71, Training loss: 6480.838088989258\n",
      "Epoch: 72, Training loss: 6603.09260559082\n",
      "Epoch: 73, Training loss: 6318.982192993164\n",
      "Epoch: 74, Training loss: 6294.043426513672\n",
      "Epoch: 75, Training loss: 6575.893112182617\n",
      "Epoch: 76, Training loss: 6450.065093994141\n",
      "Epoch: 77, Training loss: 6424.164901733398\n",
      "Epoch: 78, Training loss: 6504.754318237305\n",
      "Epoch: 79, Training loss: 6615.954208374023\n",
      "Epoch: 80, Training loss: 6317.992416381836\n",
      "Epoch: 81, Training loss: 6517.486663818359\n",
      "Epoch: 82, Training loss: 6408.704055786133\n",
      "Epoch: 83, Training loss: 6456.090057373047\n",
      "Epoch: 84, Training loss: 6613.291778564453\n",
      "Epoch: 85, Training loss: 6215.075134277344\n",
      "Epoch: 86, Training loss: 6389.275009155273\n",
      "Epoch: 87, Training loss: 6344.599830627441\n",
      "Epoch: 88, Training loss: 6346.0877685546875\n",
      "Epoch: 89, Training loss: 6403.219703674316\n",
      "Epoch: 90, Training loss: 6527.747146606445\n",
      "Epoch: 91, Training loss: 6530.588470458984\n",
      "Epoch: 92, Training loss: 6437.339500427246\n",
      "Epoch: 93, Training loss: 6462.325286865234\n",
      "Epoch: 94, Training loss: 6461.078262329102\n",
      "Epoch: 95, Training loss: 6429.739990234375\n",
      "Epoch: 96, Training loss: 6582.557678222656\n",
      "Epoch: 97, Training loss: 6679.189117431641\n",
      "Epoch: 98, Training loss: 6394.325225830078\n",
      "Epoch: 99, Training loss: 6466.254364013672\n",
      "Epoch: 100, Training loss: 6560.383377075195\n"
     ]
    }
   ],
   "source": [
    "validate_loss(modelo,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(modelo, num_epochs):\n",
    "    print(\"Training Loss \\n\")\n",
    "    training_loop(num_epochs,modelo)\n",
    "    print(\"Validation Loss \\n\")\n",
    "    validate_loss(modelo, num_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss \n",
      "\n",
      "Epoch: 1, Training loss: 2085.6604595184326\n",
      "Epoch: 2, Training loss: 1735.1494903564453\n",
      "Epoch: 3, Training loss: 1659.1402189731598\n",
      "Epoch: 4, Training loss: 1620.7659931182861\n",
      "Epoch: 5, Training loss: 1581.4648325443268\n",
      "Epoch: 6, Training loss: 1524.8351647853851\n",
      "Epoch: 7, Training loss: 1494.042650461197\n",
      "Epoch: 8, Training loss: 1478.6120176315308\n",
      "Epoch: 9, Training loss: 1406.2101775407791\n",
      "Epoch: 10, Training loss: 1402.3128443956375\n",
      "Epoch: 11, Training loss: 1408.8534848690033\n",
      "Epoch: 12, Training loss: 1379.9552167654037\n",
      "Epoch: 13, Training loss: 1331.7767465114594\n",
      "Epoch: 14, Training loss: 1329.9608577489853\n",
      "Epoch: 15, Training loss: 1316.4384591579437\n",
      "Epoch: 16, Training loss: 1283.6990307569504\n",
      "Epoch: 17, Training loss: 1291.7091102600098\n",
      "Epoch: 18, Training loss: 1253.7614606618881\n",
      "Epoch: 19, Training loss: 1203.88631606102\n",
      "Epoch: 20, Training loss: 1238.045284628868\n",
      "Epoch: 21, Training loss: 1187.355516910553\n",
      "Epoch: 22, Training loss: 1192.0408046245575\n",
      "Epoch: 23, Training loss: 1178.907680273056\n",
      "Epoch: 24, Training loss: 1129.655411362648\n",
      "Epoch: 25, Training loss: 1173.9852557182312\n",
      "Epoch: 26, Training loss: 1119.7970958948135\n",
      "Epoch: 27, Training loss: 1151.9052938222885\n",
      "Epoch: 28, Training loss: 1088.4137415885925\n",
      "Epoch: 29, Training loss: 1108.6548503637314\n",
      "Epoch: 30, Training loss: 1089.9526559114456\n",
      "Epoch: 31, Training loss: 1080.775283575058\n",
      "Epoch: 32, Training loss: 1081.7096674442291\n",
      "Epoch: 33, Training loss: 1064.2015182971954\n",
      "Epoch: 34, Training loss: 1050.4036514163017\n",
      "Epoch: 35, Training loss: 1038.0240596532822\n",
      "Epoch: 36, Training loss: 1080.857562661171\n",
      "Epoch: 37, Training loss: 1040.5207246541977\n",
      "Epoch: 38, Training loss: 1026.0099974870682\n",
      "Epoch: 39, Training loss: 1024.1084257364273\n",
      "Epoch: 40, Training loss: 987.0872687101364\n",
      "Epoch: 41, Training loss: 1031.7644919157028\n",
      "Epoch: 42, Training loss: 984.4367195367813\n",
      "Epoch: 43, Training loss: 1012.1542115211487\n",
      "Epoch: 44, Training loss: 971.8648911714554\n",
      "Epoch: 45, Training loss: 990.9934937953949\n",
      "Epoch: 46, Training loss: 952.1861577033997\n",
      "Epoch: 47, Training loss: 972.8031123876572\n",
      "Epoch: 48, Training loss: 960.0642784833908\n",
      "Epoch: 49, Training loss: 952.193633556366\n",
      "Epoch: 50, Training loss: 956.4237003326416\n",
      "Epoch: 51, Training loss: 950.3423733711243\n",
      "Epoch: 52, Training loss: 954.8477956056595\n",
      "Epoch: 53, Training loss: 950.0525122880936\n",
      "Epoch: 54, Training loss: 928.8772373199463\n",
      "Epoch: 55, Training loss: 909.8063529729843\n",
      "Epoch: 56, Training loss: 935.9644713401794\n",
      "Epoch: 57, Training loss: 902.4821743965149\n",
      "Epoch: 58, Training loss: 907.8094663619995\n",
      "Epoch: 59, Training loss: 902.5916203260422\n",
      "Epoch: 60, Training loss: 879.8873275518417\n",
      "Epoch: 61, Training loss: 870.8688759803772\n",
      "Epoch: 62, Training loss: 887.9588750600815\n",
      "Epoch: 63, Training loss: 879.367003262043\n",
      "Epoch: 64, Training loss: 857.4902721643448\n",
      "Epoch: 65, Training loss: 886.2146612405777\n",
      "Epoch: 66, Training loss: 875.81878054142\n",
      "Epoch: 67, Training loss: 843.1365636587143\n",
      "Epoch: 68, Training loss: 845.9178864955902\n",
      "Epoch: 69, Training loss: 851.0684424638748\n",
      "Epoch: 70, Training loss: 860.1198389530182\n",
      "Epoch: 71, Training loss: 838.7362769246101\n",
      "Epoch: 72, Training loss: 823.3171218633652\n",
      "Epoch: 73, Training loss: 845.3884494304657\n",
      "Epoch: 74, Training loss: 852.8542915582657\n",
      "Epoch: 75, Training loss: 841.1995388269424\n",
      "Epoch: 76, Training loss: 825.2887145280838\n",
      "Epoch: 77, Training loss: 843.6419888734818\n",
      "Epoch: 78, Training loss: 807.1434604525566\n",
      "Epoch: 79, Training loss: 804.8964041471481\n",
      "Epoch: 80, Training loss: 814.8307467699051\n",
      "Epoch: 81, Training loss: 802.3103759288788\n",
      "Epoch: 82, Training loss: 792.391483604908\n",
      "Epoch: 83, Training loss: 797.4857039451599\n",
      "Epoch: 84, Training loss: 806.7405826449394\n",
      "Epoch: 85, Training loss: 793.1513020992279\n",
      "Epoch: 86, Training loss: 792.3433421254158\n",
      "Epoch: 87, Training loss: 743.5384688973427\n",
      "Epoch: 88, Training loss: 782.2903920412064\n",
      "Epoch: 89, Training loss: 780.8171808719635\n",
      "Epoch: 90, Training loss: 791.8322222232819\n",
      "Epoch: 91, Training loss: 775.551869571209\n",
      "Epoch: 92, Training loss: 776.4665451049805\n",
      "Epoch: 93, Training loss: 764.4466512203217\n",
      "Epoch: 94, Training loss: 753.9512455463409\n",
      "Epoch: 95, Training loss: 787.9321199655533\n",
      "Epoch: 96, Training loss: 753.6519024968147\n",
      "Epoch: 97, Training loss: 727.2995211482048\n",
      "Epoch: 98, Training loss: 764.1065928936005\n",
      "Epoch: 99, Training loss: 760.3562740087509\n",
      "Epoch: 100, Training loss: 743.5138519406319\n",
      "Epoch: 101, Training loss: 739.2175427675247\n",
      "Epoch: 102, Training loss: 741.0936389565468\n",
      "Epoch: 103, Training loss: 758.2233353257179\n",
      "Epoch: 104, Training loss: 701.6152004599571\n",
      "Epoch: 105, Training loss: 737.7742128968239\n",
      "Epoch: 106, Training loss: 720.7019120454788\n",
      "Epoch: 107, Training loss: 729.1401439905167\n",
      "Epoch: 108, Training loss: 727.1511244177818\n",
      "Epoch: 109, Training loss: 710.776783645153\n",
      "Epoch: 110, Training loss: 710.5891373753548\n",
      "Epoch: 111, Training loss: 708.1326070427895\n",
      "Epoch: 112, Training loss: 688.7720205187798\n",
      "Epoch: 113, Training loss: 731.4397029876709\n",
      "Epoch: 114, Training loss: 706.1298568248749\n",
      "Epoch: 115, Training loss: 692.5771557688713\n",
      "Epoch: 116, Training loss: 720.3777284622192\n",
      "Epoch: 117, Training loss: 701.4539856910706\n",
      "Epoch: 118, Training loss: 707.4617187976837\n",
      "Epoch: 119, Training loss: 711.940648317337\n",
      "Epoch: 120, Training loss: 691.449649989605\n",
      "Epoch: 121, Training loss: 696.9621787071228\n",
      "Epoch: 122, Training loss: 687.9318051338196\n",
      "Epoch: 123, Training loss: 690.8983074426651\n",
      "Epoch: 124, Training loss: 698.4642166495323\n",
      "Epoch: 125, Training loss: 691.1454906463623\n",
      "Epoch: 126, Training loss: 664.2475584745407\n",
      "Epoch: 127, Training loss: 705.4406954050064\n",
      "Epoch: 128, Training loss: 674.4670795798302\n",
      "Epoch: 129, Training loss: 682.9481828808784\n",
      "Epoch: 130, Training loss: 676.3626614809036\n",
      "Epoch: 131, Training loss: 660.1446524262428\n",
      "Epoch: 132, Training loss: 661.3323051929474\n",
      "Epoch: 133, Training loss: 664.6392218470573\n",
      "Epoch: 134, Training loss: 668.2468311786652\n",
      "Epoch: 135, Training loss: 675.1221978068352\n",
      "Epoch: 136, Training loss: 668.5780000686646\n",
      "Epoch: 137, Training loss: 672.4903362989426\n",
      "Epoch: 138, Training loss: 653.4830253720284\n",
      "Epoch: 139, Training loss: 661.7746568918228\n",
      "Epoch: 140, Training loss: 664.6950146555901\n",
      "Epoch: 141, Training loss: 664.5565514564514\n",
      "Epoch: 142, Training loss: 639.4456396698952\n",
      "Epoch: 143, Training loss: 659.443165063858\n",
      "Epoch: 144, Training loss: 650.0111585259438\n",
      "Epoch: 145, Training loss: 653.8254470229149\n",
      "Epoch: 146, Training loss: 630.2307630777359\n",
      "Epoch: 147, Training loss: 631.5182518959045\n",
      "Epoch: 148, Training loss: 636.3604207038879\n",
      "Epoch: 149, Training loss: 648.7988195419312\n",
      "Epoch: 150, Training loss: 630.4392818212509\n",
      "Epoch: 151, Training loss: 634.9032452106476\n",
      "Epoch: 152, Training loss: 635.9841541647911\n",
      "Epoch: 153, Training loss: 646.3933897614479\n",
      "Epoch: 154, Training loss: 661.2806598544121\n",
      "Epoch: 155, Training loss: 641.1835783720016\n",
      "Epoch: 156, Training loss: 611.1118522286415\n",
      "Epoch: 157, Training loss: 626.5003020167351\n",
      "Epoch: 158, Training loss: 611.6450687050819\n",
      "Epoch: 159, Training loss: 604.0702519416809\n",
      "Epoch: 160, Training loss: 612.847655415535\n",
      "Epoch: 161, Training loss: 610.674676656723\n",
      "Epoch: 162, Training loss: 616.4250670671463\n",
      "Epoch: 163, Training loss: 617.2076026797295\n",
      "Epoch: 164, Training loss: 617.3332195878029\n",
      "Epoch: 165, Training loss: 603.9438621997833\n",
      "Epoch: 166, Training loss: 611.3883872032166\n",
      "Epoch: 167, Training loss: 621.9190354943275\n",
      "Epoch: 168, Training loss: 616.0735169649124\n",
      "Epoch: 169, Training loss: 625.6014369726181\n",
      "Epoch: 170, Training loss: 600.5410278439522\n",
      "Epoch: 171, Training loss: 604.0144974589348\n",
      "Epoch: 172, Training loss: 612.4734098911285\n",
      "Epoch: 173, Training loss: 602.8220531344414\n",
      "Epoch: 174, Training loss: 601.7171320915222\n",
      "Epoch: 175, Training loss: 593.7223781943321\n",
      "Epoch: 176, Training loss: 601.3733223080635\n",
      "Epoch: 177, Training loss: 591.2091679573059\n",
      "Epoch: 178, Training loss: 612.0391131043434\n",
      "Epoch: 179, Training loss: 611.285545527935\n",
      "Epoch: 180, Training loss: 605.8197509050369\n",
      "Epoch: 181, Training loss: 602.748226583004\n",
      "Epoch: 182, Training loss: 597.0981090664864\n",
      "Epoch: 183, Training loss: 589.0638049840927\n",
      "Epoch: 184, Training loss: 608.7463303804398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185, Training loss: 585.9747380614281\n",
      "Epoch: 186, Training loss: 572.422801554203\n",
      "Epoch: 187, Training loss: 587.05081397295\n",
      "Epoch: 188, Training loss: 583.4391450881958\n",
      "Epoch: 189, Training loss: 573.7479212284088\n",
      "Epoch: 190, Training loss: 574.7949761748314\n",
      "Epoch: 191, Training loss: 595.0482039451599\n",
      "Epoch: 192, Training loss: 569.4979938268661\n",
      "Epoch: 193, Training loss: 567.2333047389984\n",
      "Epoch: 194, Training loss: 581.0212769508362\n",
      "Epoch: 195, Training loss: 570.1785442829132\n",
      "Epoch: 196, Training loss: 578.0650225877762\n",
      "Epoch: 197, Training loss: 583.4060086011887\n",
      "Epoch: 198, Training loss: 592.924457192421\n",
      "Epoch: 199, Training loss: 572.122420668602\n",
      "Epoch: 200, Training loss: 572.9657335877419\n",
      "Epoch: 201, Training loss: 571.1164484024048\n",
      "Epoch: 202, Training loss: 571.6555458903313\n",
      "Epoch: 203, Training loss: 560.4100093841553\n",
      "Epoch: 204, Training loss: 565.8136438727379\n",
      "Epoch: 205, Training loss: 559.6275952458382\n",
      "Epoch: 206, Training loss: 546.7476889491081\n",
      "Epoch: 207, Training loss: 555.4214305281639\n",
      "Epoch: 208, Training loss: 562.1288956999779\n",
      "Epoch: 209, Training loss: 550.2909491062164\n",
      "Epoch: 210, Training loss: 551.4001352190971\n",
      "Epoch: 211, Training loss: 546.6434201598167\n",
      "Epoch: 212, Training loss: 560.6104996800423\n",
      "Epoch: 213, Training loss: 539.6083821058273\n",
      "Epoch: 214, Training loss: 546.2370818853378\n",
      "Epoch: 215, Training loss: 534.9362649917603\n",
      "Epoch: 216, Training loss: 538.300660431385\n",
      "Epoch: 217, Training loss: 552.8972405791283\n",
      "Epoch: 218, Training loss: 536.6848334670067\n",
      "Epoch: 219, Training loss: 552.017699599266\n",
      "Epoch: 220, Training loss: 533.0593851208687\n",
      "Epoch: 221, Training loss: 541.5694271326065\n",
      "Epoch: 222, Training loss: 530.9105742573738\n",
      "Epoch: 223, Training loss: 544.9172992110252\n",
      "Epoch: 224, Training loss: 541.0561955571175\n",
      "Epoch: 225, Training loss: 521.5735364556313\n",
      "Epoch: 226, Training loss: 529.2398027181625\n",
      "Epoch: 227, Training loss: 521.8799647688866\n",
      "Epoch: 228, Training loss: 544.5474436879158\n",
      "Epoch: 229, Training loss: 534.6155397891998\n",
      "Epoch: 230, Training loss: 532.2192000746727\n",
      "Epoch: 231, Training loss: 528.2028692364693\n",
      "Epoch: 232, Training loss: 515.5264192819595\n",
      "Epoch: 233, Training loss: 531.3006437420845\n",
      "Epoch: 234, Training loss: 515.237763106823\n",
      "Epoch: 235, Training loss: 536.1716420650482\n",
      "Epoch: 236, Training loss: 524.4289292693138\n",
      "Epoch: 237, Training loss: 528.9026676416397\n",
      "Epoch: 238, Training loss: 524.3274168372154\n",
      "Epoch: 239, Training loss: 518.4476570487022\n",
      "Epoch: 240, Training loss: 520.0396962761879\n",
      "Epoch: 241, Training loss: 520.4365346431732\n",
      "Epoch: 242, Training loss: 511.257100045681\n",
      "Epoch: 243, Training loss: 519.1894180178642\n",
      "Epoch: 244, Training loss: 509.48190957307816\n",
      "Epoch: 245, Training loss: 507.6481841802597\n",
      "Epoch: 246, Training loss: 517.5092303752899\n",
      "Epoch: 247, Training loss: 534.7597360014915\n",
      "Epoch: 248, Training loss: 509.00745791196823\n",
      "Epoch: 249, Training loss: 506.16648572683334\n",
      "Epoch: 250, Training loss: 523.8333206474781\n",
      "Epoch: 251, Training loss: 499.4735988378525\n",
      "Epoch: 252, Training loss: 501.40636813640594\n",
      "Epoch: 253, Training loss: 507.12542831897736\n",
      "Epoch: 254, Training loss: 495.412613928318\n",
      "Epoch: 255, Training loss: 502.10567182302475\n",
      "Epoch: 256, Training loss: 511.5544450581074\n",
      "Epoch: 257, Training loss: 500.0869019627571\n",
      "Epoch: 258, Training loss: 503.15567314624786\n",
      "Epoch: 259, Training loss: 504.234072804451\n",
      "Epoch: 260, Training loss: 501.5428104400635\n",
      "Epoch: 261, Training loss: 488.2674715220928\n",
      "Epoch: 262, Training loss: 497.90642523765564\n",
      "Epoch: 263, Training loss: 497.0562938451767\n",
      "Epoch: 264, Training loss: 506.5672281384468\n",
      "Epoch: 265, Training loss: 481.1982664167881\n",
      "Epoch: 266, Training loss: 502.9755737185478\n",
      "Epoch: 267, Training loss: 500.66556000709534\n",
      "Epoch: 268, Training loss: 498.22439646720886\n",
      "Epoch: 269, Training loss: 494.70009249448776\n",
      "Epoch: 270, Training loss: 483.15536111593246\n",
      "Epoch: 271, Training loss: 509.55535620450974\n",
      "Epoch: 272, Training loss: 501.2073876261711\n",
      "Epoch: 273, Training loss: 486.07004910707474\n",
      "Epoch: 274, Training loss: 485.6281786561012\n",
      "Epoch: 275, Training loss: 481.7321575284004\n",
      "Epoch: 276, Training loss: 501.6114158630371\n",
      "Epoch: 277, Training loss: 489.77651065588\n",
      "Epoch: 278, Training loss: 483.96353590488434\n",
      "Epoch: 279, Training loss: 485.23891711235046\n",
      "Epoch: 280, Training loss: 492.3394168615341\n",
      "Epoch: 281, Training loss: 484.23422330617905\n",
      "Epoch: 282, Training loss: 471.36207753419876\n",
      "Epoch: 283, Training loss: 472.627176463604\n",
      "Epoch: 284, Training loss: 472.73538142442703\n",
      "Epoch: 285, Training loss: 489.6918354034424\n",
      "Epoch: 286, Training loss: 485.34349024295807\n",
      "Epoch: 287, Training loss: 478.57819402217865\n",
      "Epoch: 288, Training loss: 465.84367990493774\n",
      "Epoch: 289, Training loss: 477.05669367313385\n",
      "Epoch: 290, Training loss: 475.9559505879879\n",
      "Epoch: 291, Training loss: 480.0261883735657\n",
      "Epoch: 292, Training loss: 475.2599709033966\n",
      "Epoch: 293, Training loss: 470.9350736737251\n",
      "Epoch: 294, Training loss: 475.0740170478821\n",
      "Epoch: 295, Training loss: 466.15561079978943\n",
      "Epoch: 296, Training loss: 475.3562253713608\n",
      "Epoch: 297, Training loss: 482.0088863968849\n",
      "Epoch: 298, Training loss: 472.2713066339493\n",
      "Epoch: 299, Training loss: 475.64820128679276\n",
      "Epoch: 300, Training loss: 451.89250046014786\n",
      "Epoch: 301, Training loss: 474.7389807701111\n",
      "Epoch: 302, Training loss: 462.1139788031578\n",
      "Epoch: 303, Training loss: 462.1181194782257\n",
      "Epoch: 304, Training loss: 459.78798562288284\n",
      "Epoch: 305, Training loss: 470.48305356502533\n",
      "Epoch: 306, Training loss: 461.2280229330063\n",
      "Epoch: 307, Training loss: 453.7309944331646\n",
      "Epoch: 308, Training loss: 459.9621676206589\n",
      "Epoch: 309, Training loss: 463.177543848753\n",
      "Epoch: 310, Training loss: 465.21882078051567\n",
      "Epoch: 311, Training loss: 466.85239362716675\n",
      "Epoch: 312, Training loss: 466.5920117497444\n",
      "Epoch: 313, Training loss: 452.9336322546005\n",
      "Epoch: 314, Training loss: 461.11332046985626\n",
      "Epoch: 315, Training loss: 458.4836977124214\n",
      "Epoch: 316, Training loss: 447.33872574567795\n",
      "Epoch: 317, Training loss: 457.61291044950485\n",
      "Epoch: 318, Training loss: 454.28167420625687\n",
      "Epoch: 319, Training loss: 456.8369241952896\n",
      "Epoch: 320, Training loss: 451.92613911628723\n",
      "Epoch: 321, Training loss: 446.9108520746231\n",
      "Epoch: 322, Training loss: 459.01101237535477\n",
      "Epoch: 323, Training loss: 442.7124079465866\n",
      "Epoch: 324, Training loss: 465.48221069574356\n",
      "Epoch: 325, Training loss: 450.8312318325043\n",
      "Epoch: 326, Training loss: 460.6985082626343\n",
      "Epoch: 327, Training loss: 455.7057900428772\n",
      "Epoch: 328, Training loss: 435.690564006567\n",
      "Epoch: 329, Training loss: 442.9162532091141\n",
      "Epoch: 330, Training loss: 439.65053111314774\n",
      "Epoch: 331, Training loss: 454.0270345211029\n",
      "Epoch: 332, Training loss: 444.6698040664196\n",
      "Epoch: 333, Training loss: 436.6520173251629\n",
      "Epoch: 334, Training loss: 443.4385581612587\n",
      "Epoch: 335, Training loss: 436.8104370832443\n",
      "Epoch: 336, Training loss: 457.7886045575142\n",
      "Epoch: 337, Training loss: 441.58956176042557\n",
      "Epoch: 338, Training loss: 447.380941927433\n",
      "Epoch: 339, Training loss: 443.86182457208633\n",
      "Epoch: 340, Training loss: 440.02264779806137\n",
      "Epoch: 341, Training loss: 440.26429057121277\n",
      "Epoch: 342, Training loss: 444.7109222114086\n",
      "Epoch: 343, Training loss: 439.2440309524536\n",
      "Epoch: 344, Training loss: 428.7386008501053\n",
      "Epoch: 345, Training loss: 441.5735009908676\n",
      "Epoch: 346, Training loss: 436.915799677372\n",
      "Epoch: 347, Training loss: 447.3997323513031\n",
      "Epoch: 348, Training loss: 448.0471805036068\n",
      "Epoch: 349, Training loss: 451.5303550362587\n",
      "Epoch: 350, Training loss: 432.5903776586056\n",
      "Epoch: 351, Training loss: 432.60144448280334\n",
      "Epoch: 352, Training loss: 432.4693149924278\n",
      "Epoch: 353, Training loss: 438.37682765722275\n",
      "Epoch: 354, Training loss: 442.29812455177307\n",
      "Epoch: 355, Training loss: 431.6743737459183\n",
      "Epoch: 356, Training loss: 436.42631435394287\n",
      "Epoch: 357, Training loss: 415.5999637246132\n",
      "Epoch: 358, Training loss: 421.62822061777115\n",
      "Epoch: 359, Training loss: 449.5492558181286\n",
      "Epoch: 360, Training loss: 420.06153428554535\n",
      "Epoch: 361, Training loss: 413.7554152905941\n",
      "Epoch: 362, Training loss: 426.44362235069275\n",
      "Epoch: 363, Training loss: 423.46319687366486\n",
      "Epoch: 364, Training loss: 428.11859238147736\n",
      "Epoch: 365, Training loss: 423.4668252170086\n",
      "Epoch: 366, Training loss: 434.67086070775986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 367, Training loss: 424.6317209005356\n",
      "Epoch: 368, Training loss: 425.3783462047577\n",
      "Epoch: 369, Training loss: 428.1928572654724\n",
      "Epoch: 370, Training loss: 420.02260863780975\n",
      "Epoch: 371, Training loss: 413.03854936361313\n",
      "Epoch: 372, Training loss: 422.176310300827\n",
      "Epoch: 373, Training loss: 413.2971932888031\n",
      "Epoch: 374, Training loss: 411.63712245225906\n",
      "Epoch: 375, Training loss: 426.622866332531\n",
      "Epoch: 376, Training loss: 413.8206519186497\n",
      "Epoch: 377, Training loss: 412.3495995402336\n",
      "Epoch: 378, Training loss: 419.94186186790466\n",
      "Epoch: 379, Training loss: 427.8388223052025\n",
      "Epoch: 380, Training loss: 410.60394537448883\n",
      "Epoch: 381, Training loss: 401.48171439766884\n",
      "Epoch: 382, Training loss: 428.3970694541931\n",
      "Epoch: 383, Training loss: 419.59363359212875\n",
      "Epoch: 384, Training loss: 422.2445711493492\n",
      "Epoch: 385, Training loss: 407.64861983060837\n",
      "Epoch: 386, Training loss: 413.79597222805023\n",
      "Epoch: 387, Training loss: 412.0414134860039\n",
      "Epoch: 388, Training loss: 416.9840533733368\n",
      "Epoch: 389, Training loss: 400.9312604367733\n",
      "Epoch: 390, Training loss: 428.8801575899124\n",
      "Epoch: 391, Training loss: 412.4881218075752\n",
      "Epoch: 392, Training loss: 396.1840845942497\n",
      "Epoch: 393, Training loss: 412.17554944753647\n",
      "Epoch: 394, Training loss: 409.13029938936234\n",
      "Epoch: 395, Training loss: 411.1825475692749\n",
      "Epoch: 396, Training loss: 408.7690669298172\n",
      "Epoch: 397, Training loss: 398.9603890776634\n",
      "Epoch: 398, Training loss: 399.8144124150276\n",
      "Epoch: 399, Training loss: 410.82461780309677\n",
      "Epoch: 400, Training loss: 402.23315650224686\n",
      "Epoch: 401, Training loss: 399.04225704073906\n",
      "Epoch: 402, Training loss: 408.53176432847977\n",
      "Epoch: 403, Training loss: 402.1923436522484\n",
      "Epoch: 404, Training loss: 409.9509751200676\n",
      "Epoch: 405, Training loss: 406.36514979600906\n",
      "Epoch: 406, Training loss: 405.3273763358593\n",
      "Epoch: 407, Training loss: 412.9422653913498\n",
      "Epoch: 408, Training loss: 419.8973274230957\n",
      "Epoch: 409, Training loss: 402.18430721759796\n",
      "Epoch: 410, Training loss: 409.6438422501087\n",
      "Epoch: 411, Training loss: 403.07744777202606\n",
      "Epoch: 412, Training loss: 406.7260871231556\n",
      "Epoch: 413, Training loss: 400.8255367279053\n",
      "Epoch: 414, Training loss: 412.29434394836426\n",
      "Epoch: 415, Training loss: 410.96861028671265\n",
      "Epoch: 416, Training loss: 399.15056478977203\n",
      "Epoch: 417, Training loss: 397.2673648893833\n",
      "Epoch: 418, Training loss: 398.5143565237522\n",
      "Epoch: 419, Training loss: 399.4626985192299\n",
      "Epoch: 420, Training loss: 397.55991262197495\n",
      "Epoch: 421, Training loss: 399.93490612506866\n",
      "Epoch: 422, Training loss: 396.89419770240784\n",
      "Epoch: 423, Training loss: 399.8779975771904\n",
      "Epoch: 424, Training loss: 385.8001915514469\n",
      "Epoch: 425, Training loss: 392.3202545642853\n",
      "Epoch: 426, Training loss: 392.32792216539383\n",
      "Epoch: 427, Training loss: 398.52323988080025\n",
      "Epoch: 428, Training loss: 393.1060200929642\n",
      "Epoch: 429, Training loss: 401.1429733633995\n",
      "Epoch: 430, Training loss: 385.4085998535156\n",
      "Epoch: 431, Training loss: 396.9526947438717\n",
      "Epoch: 432, Training loss: 398.77335318922997\n",
      "Epoch: 433, Training loss: 398.78484058380127\n",
      "Epoch: 434, Training loss: 394.54137176275253\n",
      "Epoch: 435, Training loss: 398.0084701180458\n",
      "Epoch: 436, Training loss: 384.888001203537\n",
      "Epoch: 437, Training loss: 393.10252115130424\n",
      "Epoch: 438, Training loss: 389.37108224630356\n",
      "Epoch: 439, Training loss: 385.73566964268684\n",
      "Epoch: 440, Training loss: 378.2117917239666\n",
      "Epoch: 441, Training loss: 386.0133869946003\n",
      "Epoch: 442, Training loss: 387.7420835494995\n",
      "Epoch: 443, Training loss: 404.9308604001999\n",
      "Epoch: 444, Training loss: 392.6387731730938\n",
      "Epoch: 445, Training loss: 379.2409661412239\n",
      "Epoch: 446, Training loss: 369.41851541399956\n",
      "Epoch: 447, Training loss: 373.51629814505577\n",
      "Epoch: 448, Training loss: 378.8663963973522\n",
      "Epoch: 449, Training loss: 381.12095934152603\n",
      "Epoch: 450, Training loss: 381.6980233192444\n",
      "Epoch: 451, Training loss: 382.49431228637695\n",
      "Epoch: 452, Training loss: 368.5069617033005\n",
      "Epoch: 453, Training loss: 384.3843497633934\n",
      "Epoch: 454, Training loss: 370.953495323658\n",
      "Epoch: 455, Training loss: 389.99115267395973\n",
      "Epoch: 456, Training loss: 389.28855633735657\n",
      "Epoch: 457, Training loss: 382.3384222984314\n",
      "Epoch: 458, Training loss: 374.8468310236931\n",
      "Epoch: 459, Training loss: 374.1440426707268\n",
      "Epoch: 460, Training loss: 387.23008954524994\n",
      "Epoch: 461, Training loss: 392.74123054742813\n",
      "Epoch: 462, Training loss: 372.75838685035706\n",
      "Epoch: 463, Training loss: 373.0949274301529\n",
      "Epoch: 464, Training loss: 381.43976014852524\n",
      "Epoch: 465, Training loss: 378.41480988264084\n",
      "Epoch: 466, Training loss: 372.9949855208397\n",
      "Epoch: 467, Training loss: 359.70440754294395\n",
      "Epoch: 468, Training loss: 374.8183073401451\n",
      "Epoch: 469, Training loss: 372.7383394241333\n",
      "Epoch: 470, Training loss: 378.03754153847694\n",
      "Epoch: 471, Training loss: 367.532905459404\n",
      "Epoch: 472, Training loss: 377.8415781855583\n",
      "Epoch: 473, Training loss: 376.0003430843353\n",
      "Epoch: 474, Training loss: 381.37116703391075\n",
      "Epoch: 475, Training loss: 377.7902339696884\n",
      "Epoch: 476, Training loss: 377.0516087114811\n",
      "Epoch: 477, Training loss: 368.79788359999657\n",
      "Epoch: 478, Training loss: 367.79911121726036\n",
      "Epoch: 479, Training loss: 372.78106331825256\n",
      "Epoch: 480, Training loss: 377.8173464536667\n",
      "Epoch: 481, Training loss: 374.7116906642914\n",
      "Epoch: 482, Training loss: 375.9085684120655\n",
      "Epoch: 483, Training loss: 379.8783751130104\n",
      "Epoch: 484, Training loss: 370.6778066456318\n",
      "Epoch: 485, Training loss: 365.56859588623047\n",
      "Epoch: 486, Training loss: 359.09015396237373\n",
      "Epoch: 487, Training loss: 361.7009322345257\n",
      "Epoch: 488, Training loss: 366.69542118906975\n",
      "Epoch: 489, Training loss: 375.25208723545074\n",
      "Epoch: 490, Training loss: 367.7443887293339\n",
      "Epoch: 491, Training loss: 376.6337157189846\n",
      "Epoch: 492, Training loss: 361.23532593250275\n",
      "Epoch: 493, Training loss: 369.47202891111374\n",
      "Epoch: 494, Training loss: 370.3822771906853\n",
      "Epoch: 495, Training loss: 373.7918685078621\n",
      "Epoch: 496, Training loss: 367.3497446477413\n",
      "Epoch: 497, Training loss: 368.9752490222454\n",
      "Epoch: 498, Training loss: 373.4982466697693\n",
      "Epoch: 499, Training loss: 365.08425319194794\n",
      "Epoch: 500, Training loss: 359.91664868593216\n",
      "Epoch: 501, Training loss: 359.68589955568314\n",
      "Epoch: 502, Training loss: 368.43605601787567\n",
      "Epoch: 503, Training loss: 368.0848636031151\n",
      "Epoch: 504, Training loss: 366.20401483774185\n",
      "Epoch: 505, Training loss: 358.152711302042\n",
      "Epoch: 506, Training loss: 369.7805194556713\n",
      "Epoch: 507, Training loss: 365.5616728067398\n",
      "Epoch: 508, Training loss: 362.07988461852074\n",
      "Epoch: 509, Training loss: 373.18260258436203\n",
      "Epoch: 510, Training loss: 367.2463691830635\n",
      "Epoch: 511, Training loss: 357.96549609303474\n",
      "Epoch: 512, Training loss: 354.757277071476\n",
      "Epoch: 513, Training loss: 367.51791620254517\n",
      "Epoch: 514, Training loss: 367.2171894609928\n",
      "Epoch: 515, Training loss: 351.77671706676483\n",
      "Epoch: 516, Training loss: 369.21392118930817\n",
      "Epoch: 517, Training loss: 362.93862664699554\n",
      "Epoch: 518, Training loss: 353.9918176829815\n",
      "Epoch: 519, Training loss: 358.0956079661846\n",
      "Epoch: 520, Training loss: 358.1850998401642\n",
      "Epoch: 521, Training loss: 354.58345741033554\n",
      "Epoch: 522, Training loss: 358.2836639881134\n",
      "Epoch: 523, Training loss: 348.6160792708397\n",
      "Epoch: 524, Training loss: 353.6683077812195\n",
      "Epoch: 525, Training loss: 347.711919516325\n",
      "Epoch: 526, Training loss: 354.4729315042496\n",
      "Epoch: 527, Training loss: 362.4443746507168\n",
      "Epoch: 528, Training loss: 359.46848544478416\n",
      "Epoch: 529, Training loss: 359.1050997674465\n",
      "Epoch: 530, Training loss: 352.1906082034111\n",
      "Epoch: 531, Training loss: 346.81672817468643\n",
      "Epoch: 532, Training loss: 353.6301322877407\n",
      "Epoch: 533, Training loss: 346.9848857522011\n",
      "Epoch: 534, Training loss: 343.6736000776291\n",
      "Epoch: 535, Training loss: 358.00537548959255\n",
      "Epoch: 536, Training loss: 347.0218192487955\n",
      "Epoch: 537, Training loss: 346.3347007930279\n",
      "Epoch: 538, Training loss: 347.904816955328\n",
      "Epoch: 539, Training loss: 359.2313888370991\n",
      "Epoch: 540, Training loss: 351.7676573097706\n",
      "Epoch: 541, Training loss: 347.88803321123123\n",
      "Epoch: 542, Training loss: 348.77514600753784\n",
      "Epoch: 543, Training loss: 349.0340195298195\n",
      "Epoch: 544, Training loss: 343.60897120833397\n",
      "Epoch: 545, Training loss: 349.9024776518345\n",
      "Epoch: 546, Training loss: 352.6365645825863\n",
      "Epoch: 547, Training loss: 351.29075342416763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 548, Training loss: 352.4235483109951\n",
      "Epoch: 549, Training loss: 348.2594003677368\n",
      "Epoch: 550, Training loss: 353.8304861187935\n",
      "Epoch: 551, Training loss: 336.631771504879\n",
      "Epoch: 552, Training loss: 342.9490233659744\n",
      "Epoch: 553, Training loss: 339.87831476330757\n",
      "Epoch: 554, Training loss: 352.73171949386597\n",
      "Epoch: 555, Training loss: 344.6084548830986\n",
      "Epoch: 556, Training loss: 332.4766064584255\n",
      "Epoch: 557, Training loss: 352.70077097415924\n",
      "Epoch: 558, Training loss: 347.7415928840637\n",
      "Epoch: 559, Training loss: 339.59621888399124\n",
      "Epoch: 560, Training loss: 348.4951182603836\n",
      "Epoch: 561, Training loss: 352.81444746255875\n",
      "Epoch: 562, Training loss: 342.164752304554\n",
      "Epoch: 563, Training loss: 346.1823875308037\n",
      "Epoch: 564, Training loss: 341.0061305165291\n",
      "Epoch: 565, Training loss: 350.48450219631195\n",
      "Epoch: 566, Training loss: 341.0992856025696\n",
      "Epoch: 567, Training loss: 339.2708585858345\n",
      "Epoch: 568, Training loss: 344.54090145230293\n",
      "Epoch: 569, Training loss: 335.479729950428\n",
      "Epoch: 570, Training loss: 340.9356691837311\n",
      "Epoch: 571, Training loss: 348.8512510359287\n",
      "Epoch: 572, Training loss: 350.4352131485939\n",
      "Epoch: 573, Training loss: 341.98675841093063\n",
      "Epoch: 574, Training loss: 329.51436907052994\n",
      "Epoch: 575, Training loss: 345.1315553486347\n",
      "Epoch: 576, Training loss: 338.1897000968456\n",
      "Epoch: 577, Training loss: 331.4979952573776\n",
      "Epoch: 578, Training loss: 333.36208164691925\n",
      "Epoch: 579, Training loss: 337.848819822073\n",
      "Epoch: 580, Training loss: 338.35761228203773\n",
      "Epoch: 581, Training loss: 347.20816445350647\n",
      "Epoch: 582, Training loss: 328.2446745932102\n",
      "Epoch: 583, Training loss: 336.9211328625679\n",
      "Epoch: 584, Training loss: 329.90288826823235\n",
      "Epoch: 585, Training loss: 336.3751847445965\n",
      "Epoch: 586, Training loss: 338.79960334300995\n",
      "Epoch: 587, Training loss: 335.9300782084465\n",
      "Epoch: 588, Training loss: 340.15229097008705\n",
      "Epoch: 589, Training loss: 346.47386398911476\n",
      "Epoch: 590, Training loss: 344.2060264647007\n",
      "Epoch: 591, Training loss: 329.2952330708504\n",
      "Epoch: 592, Training loss: 333.3701837360859\n",
      "Epoch: 593, Training loss: 328.3270890414715\n",
      "Epoch: 594, Training loss: 329.91158056259155\n",
      "Epoch: 595, Training loss: 327.3258606791496\n",
      "Epoch: 596, Training loss: 334.0944359898567\n",
      "Epoch: 597, Training loss: 339.00174736976624\n",
      "Epoch: 598, Training loss: 328.1964835822582\n",
      "Epoch: 599, Training loss: 336.714764803648\n",
      "Epoch: 600, Training loss: 334.3369147181511\n",
      "Epoch: 601, Training loss: 326.72610798478127\n",
      "Epoch: 602, Training loss: 338.6604831814766\n",
      "Epoch: 603, Training loss: 331.74886897206306\n",
      "Epoch: 604, Training loss: 326.15562602877617\n",
      "Epoch: 605, Training loss: 329.414157807827\n",
      "Epoch: 606, Training loss: 327.1416290998459\n",
      "Epoch: 607, Training loss: 325.5403284430504\n",
      "Epoch: 608, Training loss: 327.62119007110596\n",
      "Epoch: 609, Training loss: 326.1178291141987\n",
      "Epoch: 610, Training loss: 320.4919795393944\n",
      "Epoch: 611, Training loss: 327.3188524246216\n",
      "Epoch: 612, Training loss: 315.2253614962101\n",
      "Epoch: 613, Training loss: 316.984962284565\n",
      "Epoch: 614, Training loss: 327.6716622412205\n",
      "Epoch: 615, Training loss: 316.0787914991379\n",
      "Epoch: 616, Training loss: 330.42903208732605\n",
      "Epoch: 617, Training loss: 325.8399415612221\n",
      "Epoch: 618, Training loss: 319.69843274354935\n",
      "Epoch: 619, Training loss: 323.27770578861237\n",
      "Epoch: 620, Training loss: 336.32318565249443\n",
      "Epoch: 621, Training loss: 333.47955656051636\n",
      "Epoch: 622, Training loss: 330.70136016607285\n",
      "Epoch: 623, Training loss: 315.0063888132572\n",
      "Epoch: 624, Training loss: 323.4472806453705\n",
      "Epoch: 625, Training loss: 331.8881066143513\n",
      "Epoch: 626, Training loss: 324.96582609415054\n",
      "Epoch: 627, Training loss: 329.6673462688923\n",
      "Epoch: 628, Training loss: 319.07044073939323\n",
      "Epoch: 629, Training loss: 322.12834960222244\n",
      "Epoch: 630, Training loss: 306.65745627880096\n",
      "Epoch: 631, Training loss: 329.1230757832527\n",
      "Epoch: 632, Training loss: 325.39117816090584\n",
      "Epoch: 633, Training loss: 333.11570131778717\n",
      "Epoch: 634, Training loss: 320.68879437446594\n",
      "Epoch: 635, Training loss: 316.7449305653572\n",
      "Epoch: 636, Training loss: 319.34202471375465\n",
      "Epoch: 637, Training loss: 323.3510693311691\n",
      "Epoch: 638, Training loss: 321.0739340186119\n",
      "Epoch: 639, Training loss: 322.7015820145607\n",
      "Epoch: 640, Training loss: 323.27278113365173\n",
      "Epoch: 641, Training loss: 314.13442692160606\n",
      "Epoch: 642, Training loss: 319.0251253247261\n",
      "Epoch: 643, Training loss: 325.37808015942574\n",
      "Epoch: 644, Training loss: 315.3989298939705\n",
      "Epoch: 645, Training loss: 320.7262415587902\n",
      "Epoch: 646, Training loss: 310.7491606771946\n",
      "Epoch: 647, Training loss: 307.7749339938164\n",
      "Epoch: 648, Training loss: 316.80511397123337\n",
      "Epoch: 649, Training loss: 324.7001641392708\n",
      "Epoch: 650, Training loss: 316.4660404920578\n",
      "Epoch: 651, Training loss: 319.58666226267815\n",
      "Epoch: 652, Training loss: 315.59292751550674\n",
      "Epoch: 653, Training loss: 306.6173549890518\n",
      "Epoch: 654, Training loss: 315.86601880192757\n",
      "Epoch: 655, Training loss: 310.7089247703552\n",
      "Epoch: 656, Training loss: 320.5834701061249\n",
      "Epoch: 657, Training loss: 302.1524084210396\n",
      "Epoch: 658, Training loss: 305.95973268151283\n",
      "Epoch: 659, Training loss: 315.9149505198002\n",
      "Epoch: 660, Training loss: 323.2324652969837\n",
      "Epoch: 661, Training loss: 298.9887881577015\n",
      "Epoch: 662, Training loss: 306.32576617598534\n",
      "Epoch: 663, Training loss: 315.63382482528687\n",
      "Epoch: 664, Training loss: 319.7712781429291\n",
      "Epoch: 665, Training loss: 326.5708733201027\n",
      "Epoch: 666, Training loss: 306.89117535948753\n",
      "Epoch: 667, Training loss: 312.51038509607315\n",
      "Epoch: 668, Training loss: 316.8788951039314\n",
      "Epoch: 669, Training loss: 313.78034457564354\n",
      "Epoch: 670, Training loss: 316.85250437259674\n",
      "Epoch: 671, Training loss: 318.30001044273376\n",
      "Epoch: 672, Training loss: 305.92771857976913\n",
      "Epoch: 673, Training loss: 311.4182626605034\n",
      "Epoch: 674, Training loss: 318.9688607752323\n",
      "Epoch: 675, Training loss: 295.33451837301254\n",
      "Epoch: 676, Training loss: 317.1738677620888\n",
      "Epoch: 677, Training loss: 297.2556023299694\n",
      "Epoch: 678, Training loss: 311.6240821480751\n",
      "Epoch: 679, Training loss: 315.35992239415646\n",
      "Epoch: 680, Training loss: 307.8423933684826\n",
      "Epoch: 681, Training loss: 309.1113712489605\n",
      "Epoch: 682, Training loss: 311.5535489320755\n",
      "Epoch: 683, Training loss: 309.5412037372589\n",
      "Epoch: 684, Training loss: 302.6081994175911\n",
      "Epoch: 685, Training loss: 303.20299115777016\n",
      "Epoch: 686, Training loss: 303.73186844587326\n",
      "Epoch: 687, Training loss: 310.410323292017\n",
      "Epoch: 688, Training loss: 305.7251786291599\n",
      "Epoch: 689, Training loss: 313.19200390577316\n",
      "Epoch: 690, Training loss: 310.1093426346779\n",
      "Epoch: 691, Training loss: 308.3694007098675\n",
      "Epoch: 692, Training loss: 308.8687339425087\n",
      "Epoch: 693, Training loss: 299.50742143392563\n",
      "Epoch: 694, Training loss: 301.919161260128\n",
      "Epoch: 695, Training loss: 302.9402996003628\n",
      "Epoch: 696, Training loss: 304.4464200735092\n",
      "Epoch: 697, Training loss: 300.8922683596611\n",
      "Epoch: 698, Training loss: 301.69081833958626\n",
      "Epoch: 699, Training loss: 300.2727094590664\n",
      "Epoch: 700, Training loss: 302.28038772940636\n",
      "Epoch: 701, Training loss: 299.6388968527317\n",
      "Epoch: 702, Training loss: 298.82865685224533\n",
      "Epoch: 703, Training loss: 297.10438960790634\n",
      "Epoch: 704, Training loss: 304.0635878741741\n",
      "Epoch: 705, Training loss: 309.4343053996563\n",
      "Epoch: 706, Training loss: 293.0011722445488\n",
      "Epoch: 707, Training loss: 306.7980125248432\n",
      "Epoch: 708, Training loss: 300.27737709879875\n",
      "Epoch: 709, Training loss: 300.3325296640396\n",
      "Epoch: 710, Training loss: 301.45673045516014\n",
      "Epoch: 711, Training loss: 303.29662451148033\n",
      "Epoch: 712, Training loss: 296.21167039871216\n",
      "Epoch: 713, Training loss: 296.1493098139763\n",
      "Epoch: 714, Training loss: 307.22961819171906\n",
      "Epoch: 715, Training loss: 305.6468114256859\n",
      "Epoch: 716, Training loss: 295.6615583896637\n",
      "Epoch: 717, Training loss: 301.5927627980709\n",
      "Epoch: 718, Training loss: 291.3674435913563\n",
      "Epoch: 719, Training loss: 293.37350726127625\n",
      "Epoch: 720, Training loss: 300.5531870126724\n",
      "Epoch: 721, Training loss: 296.99391743540764\n",
      "Epoch: 722, Training loss: 293.4387158155441\n",
      "Epoch: 723, Training loss: 310.1057982444763\n",
      "Epoch: 724, Training loss: 304.3844790160656\n",
      "Epoch: 725, Training loss: 296.7591282427311\n",
      "Epoch: 726, Training loss: 287.06292882561684\n",
      "Epoch: 727, Training loss: 295.11240458488464\n",
      "Epoch: 728, Training loss: 306.63838973641396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 729, Training loss: 305.69656881690025\n",
      "Epoch: 730, Training loss: 298.46096831560135\n",
      "Epoch: 731, Training loss: 298.50720858573914\n",
      "Epoch: 732, Training loss: 297.8423972129822\n",
      "Epoch: 733, Training loss: 301.57943230867386\n",
      "Epoch: 734, Training loss: 303.3607243001461\n",
      "Epoch: 735, Training loss: 292.070567548275\n",
      "Epoch: 736, Training loss: 303.40859635174274\n",
      "Epoch: 737, Training loss: 295.2474056184292\n",
      "Epoch: 738, Training loss: 294.2418255805969\n",
      "Epoch: 739, Training loss: 295.2913895249367\n",
      "Epoch: 740, Training loss: 290.5724959373474\n",
      "Epoch: 741, Training loss: 302.4266848266125\n",
      "Epoch: 742, Training loss: 290.9289628267288\n",
      "Epoch: 743, Training loss: 290.7173461318016\n",
      "Epoch: 744, Training loss: 297.4541704505682\n",
      "Epoch: 745, Training loss: 294.47249814867973\n",
      "Epoch: 746, Training loss: 291.23484271764755\n",
      "Epoch: 747, Training loss: 291.2275914847851\n",
      "Epoch: 748, Training loss: 288.7187221646309\n",
      "Epoch: 749, Training loss: 294.86404529213905\n",
      "Epoch: 750, Training loss: 283.83623434603214\n",
      "Epoch: 751, Training loss: 287.54204255342484\n",
      "Epoch: 752, Training loss: 291.4899751543999\n",
      "Epoch: 753, Training loss: 294.0240446925163\n",
      "Epoch: 754, Training loss: 288.3666705787182\n",
      "Epoch: 755, Training loss: 286.556927472353\n",
      "Epoch: 756, Training loss: 286.21969187259674\n",
      "Epoch: 757, Training loss: 288.72463539242744\n",
      "Epoch: 758, Training loss: 293.5879437029362\n",
      "Epoch: 759, Training loss: 287.72972497344017\n",
      "Epoch: 760, Training loss: 293.156309068203\n",
      "Epoch: 761, Training loss: 280.08326230943203\n",
      "Epoch: 762, Training loss: 292.718377918005\n",
      "Epoch: 763, Training loss: 289.5370274782181\n",
      "Epoch: 764, Training loss: 289.6577553153038\n",
      "Epoch: 765, Training loss: 290.91804057359695\n",
      "Epoch: 766, Training loss: 290.1675278544426\n",
      "Epoch: 767, Training loss: 295.6854189634323\n",
      "Epoch: 768, Training loss: 281.20590975880623\n",
      "Epoch: 769, Training loss: 288.2759382426739\n",
      "Epoch: 770, Training loss: 294.5194718539715\n",
      "Epoch: 771, Training loss: 294.62965497374535\n",
      "Epoch: 772, Training loss: 291.76849550008774\n",
      "Epoch: 773, Training loss: 296.7845283448696\n",
      "Epoch: 774, Training loss: 288.6976494193077\n",
      "Epoch: 775, Training loss: 285.3974197804928\n",
      "Epoch: 776, Training loss: 284.47287967801094\n",
      "Epoch: 777, Training loss: 288.27659875154495\n",
      "Epoch: 778, Training loss: 279.9782526791096\n",
      "Epoch: 779, Training loss: 289.34724831581116\n",
      "Epoch: 780, Training loss: 287.1798568367958\n",
      "Epoch: 781, Training loss: 286.04165732860565\n",
      "Epoch: 782, Training loss: 281.4502217769623\n",
      "Epoch: 783, Training loss: 298.9765867292881\n",
      "Epoch: 784, Training loss: 283.3355706334114\n",
      "Epoch: 785, Training loss: 281.9582819342613\n",
      "Epoch: 786, Training loss: 286.6194944381714\n",
      "Epoch: 787, Training loss: 285.05968204140663\n",
      "Epoch: 788, Training loss: 289.0913470387459\n",
      "Epoch: 789, Training loss: 289.34681114554405\n",
      "Epoch: 790, Training loss: 279.9266673922539\n",
      "Epoch: 791, Training loss: 283.22543451189995\n",
      "Epoch: 792, Training loss: 290.9312001466751\n",
      "Epoch: 793, Training loss: 281.65184357762337\n",
      "Epoch: 794, Training loss: 294.88765475153923\n",
      "Epoch: 795, Training loss: 284.3869062960148\n",
      "Epoch: 796, Training loss: 285.5322345197201\n",
      "Epoch: 797, Training loss: 280.45023161172867\n",
      "Epoch: 798, Training loss: 275.47096186876297\n",
      "Epoch: 799, Training loss: 281.7865848839283\n",
      "Epoch: 800, Training loss: 281.1588107049465\n",
      "Epoch: 801, Training loss: 283.49596548080444\n",
      "Epoch: 802, Training loss: 278.6127519607544\n",
      "Epoch: 803, Training loss: 281.0102408826351\n",
      "Epoch: 804, Training loss: 286.23098587989807\n",
      "Epoch: 805, Training loss: 275.1930344104767\n",
      "Epoch: 806, Training loss: 286.6769549250603\n",
      "Epoch: 807, Training loss: 282.408213108778\n",
      "Epoch: 808, Training loss: 283.16339364647865\n",
      "Epoch: 809, Training loss: 269.7053319811821\n",
      "Epoch: 810, Training loss: 283.65214785933495\n",
      "Epoch: 811, Training loss: 274.55033975839615\n",
      "Epoch: 812, Training loss: 276.5064580142498\n",
      "Epoch: 813, Training loss: 283.494122415781\n",
      "Epoch: 814, Training loss: 275.6944153010845\n",
      "Epoch: 815, Training loss: 277.99077782034874\n",
      "Epoch: 816, Training loss: 281.6154661476612\n",
      "Epoch: 817, Training loss: 277.3116317987442\n",
      "Epoch: 818, Training loss: 284.7962385416031\n",
      "Epoch: 819, Training loss: 279.20548486709595\n",
      "Epoch: 820, Training loss: 270.7646566927433\n",
      "Epoch: 821, Training loss: 279.13355699181557\n",
      "Epoch: 822, Training loss: 280.2721053957939\n",
      "Epoch: 823, Training loss: 279.74796801805496\n",
      "Epoch: 824, Training loss: 273.6117227971554\n",
      "Epoch: 825, Training loss: 277.6094543635845\n",
      "Epoch: 826, Training loss: 275.68162816762924\n",
      "Epoch: 827, Training loss: 281.12835171818733\n",
      "Epoch: 828, Training loss: 274.4833950102329\n",
      "Epoch: 829, Training loss: 276.16569966077805\n",
      "Epoch: 830, Training loss: 270.9793561697006\n",
      "Epoch: 831, Training loss: 284.8705124557018\n",
      "Epoch: 832, Training loss: 274.0100120604038\n",
      "Epoch: 833, Training loss: 271.00903195142746\n",
      "Epoch: 834, Training loss: 278.1261887848377\n",
      "Epoch: 835, Training loss: 270.8257784843445\n",
      "Epoch: 836, Training loss: 259.37905037403107\n",
      "Epoch: 837, Training loss: 271.8885768651962\n",
      "Epoch: 838, Training loss: 281.1473654806614\n",
      "Epoch: 839, Training loss: 268.4327076077461\n",
      "Epoch: 840, Training loss: 274.6752316057682\n",
      "Epoch: 841, Training loss: 280.73029497265816\n",
      "Epoch: 842, Training loss: 270.91292774677277\n",
      "Epoch: 843, Training loss: 276.2163696885109\n",
      "Epoch: 844, Training loss: 273.3194525837898\n",
      "Epoch: 845, Training loss: 265.6138376891613\n",
      "Epoch: 846, Training loss: 271.17299178242683\n",
      "Epoch: 847, Training loss: 273.50642293691635\n",
      "Epoch: 848, Training loss: 269.1310995221138\n",
      "Epoch: 849, Training loss: 267.885502576828\n",
      "Epoch: 850, Training loss: 267.08703780174255\n",
      "Epoch: 851, Training loss: 270.0898254811764\n",
      "Epoch: 852, Training loss: 276.9582298696041\n",
      "Epoch: 853, Training loss: 274.87097439169884\n",
      "Epoch: 854, Training loss: 270.12727895379066\n",
      "Epoch: 855, Training loss: 269.4882418513298\n",
      "Epoch: 856, Training loss: 267.7845935225487\n",
      "Epoch: 857, Training loss: 272.04830008745193\n",
      "Epoch: 858, Training loss: 258.97074607014656\n",
      "Epoch: 859, Training loss: 276.93669033050537\n",
      "Epoch: 860, Training loss: 266.4345973730087\n",
      "Epoch: 861, Training loss: 268.52086940407753\n",
      "Epoch: 862, Training loss: 275.10066252946854\n",
      "Epoch: 863, Training loss: 282.2691878080368\n",
      "Epoch: 864, Training loss: 274.47170555591583\n",
      "Epoch: 865, Training loss: 264.4821211397648\n",
      "Epoch: 866, Training loss: 274.3787359893322\n",
      "Epoch: 867, Training loss: 269.6093572676182\n",
      "Epoch: 868, Training loss: 271.709635168314\n",
      "Epoch: 869, Training loss: 262.7465785443783\n",
      "Epoch: 870, Training loss: 267.31256234645844\n",
      "Epoch: 871, Training loss: 266.7730894088745\n",
      "Epoch: 872, Training loss: 271.4997036755085\n",
      "Epoch: 873, Training loss: 270.7588045299053\n",
      "Epoch: 874, Training loss: 268.9044114649296\n",
      "Epoch: 875, Training loss: 267.849611222744\n",
      "Epoch: 876, Training loss: 261.4256232380867\n",
      "Epoch: 877, Training loss: 271.8796860873699\n",
      "Epoch: 878, Training loss: 269.6713342666626\n",
      "Epoch: 879, Training loss: 256.4540123939514\n",
      "Epoch: 880, Training loss: 273.1162064373493\n",
      "Epoch: 881, Training loss: 260.5818998515606\n",
      "Epoch: 882, Training loss: 262.9425485432148\n",
      "Epoch: 883, Training loss: 268.30806198716164\n",
      "Epoch: 884, Training loss: 261.65203833580017\n",
      "Epoch: 885, Training loss: 265.7775319814682\n",
      "Epoch: 886, Training loss: 261.6442563831806\n",
      "Epoch: 887, Training loss: 272.5484250485897\n",
      "Epoch: 888, Training loss: 261.50012734532356\n",
      "Epoch: 889, Training loss: 267.6812670826912\n",
      "Epoch: 890, Training loss: 263.5811147391796\n",
      "Epoch: 891, Training loss: 265.61099192500114\n",
      "Epoch: 892, Training loss: 263.90935230255127\n",
      "Epoch: 893, Training loss: 267.1586028933525\n",
      "Epoch: 894, Training loss: 266.4052489697933\n",
      "Epoch: 895, Training loss: 263.17178323864937\n",
      "Epoch: 896, Training loss: 266.3063963651657\n",
      "Epoch: 897, Training loss: 264.85915318131447\n",
      "Epoch: 898, Training loss: 263.1111392378807\n",
      "Epoch: 899, Training loss: 258.1002520620823\n",
      "Epoch: 900, Training loss: 266.5343624949455\n",
      "Epoch: 901, Training loss: 267.6220197379589\n",
      "Epoch: 902, Training loss: 263.57771426439285\n",
      "Epoch: 903, Training loss: 261.7006502151489\n",
      "Epoch: 904, Training loss: 265.4920242726803\n",
      "Epoch: 905, Training loss: 264.60186687111855\n",
      "Epoch: 906, Training loss: 256.9225503504276\n",
      "Epoch: 907, Training loss: 266.04458060860634\n",
      "Epoch: 908, Training loss: 260.75315698981285\n",
      "Epoch: 909, Training loss: 252.5185604095459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 910, Training loss: 252.17701494693756\n",
      "Epoch: 911, Training loss: 265.14052625000477\n",
      "Epoch: 912, Training loss: 259.39879363775253\n",
      "Epoch: 913, Training loss: 264.7232349514961\n",
      "Epoch: 914, Training loss: 270.19557973742485\n",
      "Epoch: 915, Training loss: 256.6850001215935\n",
      "Epoch: 916, Training loss: 258.7484764456749\n",
      "Epoch: 917, Training loss: 263.45922791957855\n",
      "Epoch: 918, Training loss: 258.131274074316\n",
      "Epoch: 919, Training loss: 257.1282750964165\n",
      "Epoch: 920, Training loss: 263.5501507371664\n",
      "Epoch: 921, Training loss: 257.2671721279621\n",
      "Epoch: 922, Training loss: 254.00452011823654\n",
      "Epoch: 923, Training loss: 258.88266056776047\n",
      "Epoch: 924, Training loss: 258.1812320947647\n",
      "Epoch: 925, Training loss: 257.1159844696522\n",
      "Epoch: 926, Training loss: 264.7589115202427\n",
      "Epoch: 927, Training loss: 262.38478687405586\n",
      "Epoch: 928, Training loss: 256.7407873570919\n",
      "Epoch: 929, Training loss: 264.17896059155464\n",
      "Epoch: 930, Training loss: 256.8100737333298\n",
      "Epoch: 931, Training loss: 258.80691972374916\n",
      "Epoch: 932, Training loss: 259.2114459872246\n",
      "Epoch: 933, Training loss: 253.4760802090168\n",
      "Epoch: 934, Training loss: 258.4811978638172\n",
      "Epoch: 935, Training loss: 249.5767991244793\n",
      "Epoch: 936, Training loss: 257.8123057484627\n",
      "Epoch: 937, Training loss: 252.08921559154987\n",
      "Epoch: 938, Training loss: 259.4396107494831\n",
      "Epoch: 939, Training loss: 256.43009723722935\n",
      "Epoch: 940, Training loss: 255.8140971660614\n",
      "Epoch: 941, Training loss: 259.2253322303295\n",
      "Epoch: 942, Training loss: 257.2510476112366\n",
      "Epoch: 943, Training loss: 247.5777682363987\n",
      "Epoch: 944, Training loss: 252.9727808535099\n",
      "Epoch: 945, Training loss: 250.8938808143139\n",
      "Epoch: 946, Training loss: 258.4257646203041\n",
      "Epoch: 947, Training loss: 259.8468135893345\n",
      "Epoch: 948, Training loss: 254.58416119217873\n",
      "Epoch: 949, Training loss: 250.9344847649336\n",
      "Epoch: 950, Training loss: 257.9671287238598\n",
      "Epoch: 951, Training loss: 258.5820540189743\n",
      "Epoch: 952, Training loss: 253.35437688231468\n",
      "Epoch: 953, Training loss: 244.7597453892231\n",
      "Epoch: 954, Training loss: 253.12317553162575\n",
      "Epoch: 955, Training loss: 253.69739562273026\n",
      "Epoch: 956, Training loss: 259.4814702272415\n",
      "Epoch: 957, Training loss: 250.48932954669\n",
      "Epoch: 958, Training loss: 251.00053870677948\n",
      "Epoch: 959, Training loss: 254.80740293860435\n",
      "Epoch: 960, Training loss: 254.28805062174797\n",
      "Epoch: 961, Training loss: 252.0784436762333\n",
      "Epoch: 962, Training loss: 249.2641674876213\n",
      "Epoch: 963, Training loss: 246.151781976223\n",
      "Epoch: 964, Training loss: 244.7953487932682\n",
      "Epoch: 965, Training loss: 249.5493655204773\n",
      "Epoch: 966, Training loss: 254.31016544997692\n",
      "Epoch: 967, Training loss: 249.32251927256584\n",
      "Epoch: 968, Training loss: 255.38598054647446\n",
      "Epoch: 969, Training loss: 252.89591221511364\n",
      "Epoch: 970, Training loss: 252.60703840851784\n",
      "Epoch: 971, Training loss: 250.48084589838982\n",
      "Epoch: 972, Training loss: 253.99810937047005\n",
      "Epoch: 973, Training loss: 243.10977658629417\n",
      "Epoch: 974, Training loss: 252.97650796175003\n",
      "Epoch: 975, Training loss: 255.98046526312828\n",
      "Epoch: 976, Training loss: 256.0695349872112\n",
      "Epoch: 977, Training loss: 251.12052381038666\n",
      "Epoch: 978, Training loss: 250.34795621037483\n",
      "Epoch: 979, Training loss: 248.9030977487564\n",
      "Epoch: 980, Training loss: 257.61212976276875\n",
      "Epoch: 981, Training loss: 251.48058021068573\n",
      "Epoch: 982, Training loss: 250.86048039793968\n",
      "Epoch: 983, Training loss: 251.6549035012722\n",
      "Epoch: 984, Training loss: 257.2226056754589\n",
      "Epoch: 985, Training loss: 243.230334430933\n",
      "Epoch: 986, Training loss: 244.61427184939384\n",
      "Epoch: 987, Training loss: 256.5249501466751\n",
      "Epoch: 988, Training loss: 254.62105637788773\n",
      "Epoch: 989, Training loss: 249.994896620512\n",
      "Epoch: 990, Training loss: 256.8788599073887\n",
      "Epoch: 991, Training loss: 257.43409955501556\n",
      "Epoch: 992, Training loss: 249.04121232032776\n",
      "Epoch: 993, Training loss: 259.8704133629799\n",
      "Epoch: 994, Training loss: 251.0589835345745\n",
      "Epoch: 995, Training loss: 246.5328589975834\n",
      "Epoch: 996, Training loss: 247.71999770402908\n",
      "Epoch: 997, Training loss: 247.5855368077755\n",
      "Epoch: 998, Training loss: 250.68244694173336\n",
      "Epoch: 999, Training loss: 252.66219678521156\n",
      "Epoch: 1000, Training loss: 245.19332268834114\n",
      "Validation Loss \n",
      "\n",
      "Epoch: 1, Training loss: 23.78142324090004\n",
      "Epoch: 2, Training loss: 23.93478113412857\n",
      "Epoch: 3, Training loss: 25.529392778873444\n",
      "Epoch: 4, Training loss: 25.568142235279083\n",
      "Epoch: 5, Training loss: 22.87800559401512\n",
      "Epoch: 6, Training loss: 23.595032781362534\n",
      "Epoch: 7, Training loss: 26.155494689941406\n",
      "Epoch: 8, Training loss: 23.71052050590515\n",
      "Epoch: 9, Training loss: 23.7065207362175\n",
      "Epoch: 10, Training loss: 25.17909812927246\n",
      "Epoch: 11, Training loss: 25.082676112651825\n",
      "Epoch: 12, Training loss: 22.281129121780396\n",
      "Epoch: 13, Training loss: 25.087800204753876\n",
      "Epoch: 14, Training loss: 23.533500283956528\n",
      "Epoch: 15, Training loss: 25.360645532608032\n",
      "Epoch: 16, Training loss: 24.947054833173752\n",
      "Epoch: 17, Training loss: 25.42835885286331\n",
      "Epoch: 18, Training loss: 23.068452328443527\n",
      "Epoch: 19, Training loss: 24.40874269604683\n",
      "Epoch: 20, Training loss: 24.474426686763763\n",
      "Epoch: 21, Training loss: 24.828500390052795\n",
      "Epoch: 22, Training loss: 25.141657441854477\n",
      "Epoch: 23, Training loss: 25.700641214847565\n",
      "Epoch: 24, Training loss: 25.39303132891655\n",
      "Epoch: 25, Training loss: 24.82185187935829\n",
      "Epoch: 26, Training loss: 25.031782120466232\n",
      "Epoch: 27, Training loss: 24.00654637813568\n",
      "Epoch: 28, Training loss: 23.20531141757965\n",
      "Epoch: 29, Training loss: 24.011826813220978\n",
      "Epoch: 30, Training loss: 24.683816015720367\n",
      "Epoch: 31, Training loss: 23.29523429274559\n",
      "Epoch: 32, Training loss: 26.48004001379013\n",
      "Epoch: 33, Training loss: 23.636103004217148\n",
      "Epoch: 34, Training loss: 26.86749666929245\n",
      "Epoch: 35, Training loss: 26.38280662894249\n",
      "Epoch: 36, Training loss: 25.372654259204865\n",
      "Epoch: 37, Training loss: 24.099199652671814\n",
      "Epoch: 38, Training loss: 25.250575721263885\n",
      "Epoch: 39, Training loss: 23.805318117141724\n",
      "Epoch: 40, Training loss: 23.679033190011978\n",
      "Epoch: 41, Training loss: 27.91111984848976\n",
      "Epoch: 42, Training loss: 22.613764882087708\n",
      "Epoch: 43, Training loss: 24.11087918281555\n",
      "Epoch: 44, Training loss: 25.159158498048782\n",
      "Epoch: 45, Training loss: 24.548618376255035\n",
      "Epoch: 46, Training loss: 24.42555496096611\n",
      "Epoch: 47, Training loss: 22.47215011715889\n",
      "Epoch: 48, Training loss: 24.87792208790779\n",
      "Epoch: 49, Training loss: 23.557287275791168\n",
      "Epoch: 50, Training loss: 24.386474579572678\n",
      "Epoch: 51, Training loss: 24.9622865319252\n",
      "Epoch: 52, Training loss: 25.50495406985283\n",
      "Epoch: 53, Training loss: 23.837401181459427\n",
      "Epoch: 54, Training loss: 25.57190516591072\n",
      "Epoch: 55, Training loss: 24.31681177020073\n",
      "Epoch: 56, Training loss: 24.444188177585602\n",
      "Epoch: 57, Training loss: 24.74269700050354\n",
      "Epoch: 58, Training loss: 22.649352610111237\n",
      "Epoch: 59, Training loss: 23.32141277194023\n",
      "Epoch: 60, Training loss: 24.160695254802704\n",
      "Epoch: 61, Training loss: 25.110028862953186\n",
      "Epoch: 62, Training loss: 25.22428148984909\n",
      "Epoch: 63, Training loss: 22.746881186962128\n",
      "Epoch: 64, Training loss: 24.77136740088463\n",
      "Epoch: 65, Training loss: 23.751774698495865\n",
      "Epoch: 66, Training loss: 23.18257635831833\n",
      "Epoch: 67, Training loss: 25.29360058903694\n",
      "Epoch: 68, Training loss: 25.614068627357483\n",
      "Epoch: 69, Training loss: 26.05600482225418\n",
      "Epoch: 70, Training loss: 24.15847772359848\n",
      "Epoch: 71, Training loss: 23.22659182548523\n",
      "Epoch: 72, Training loss: 24.285088151693344\n",
      "Epoch: 73, Training loss: 23.748047530651093\n",
      "Epoch: 74, Training loss: 22.13444384932518\n",
      "Epoch: 75, Training loss: 25.96613246202469\n",
      "Epoch: 76, Training loss: 23.35696366429329\n",
      "Epoch: 77, Training loss: 24.515200972557068\n",
      "Epoch: 78, Training loss: 25.284922301769257\n",
      "Epoch: 79, Training loss: 25.561465978622437\n",
      "Epoch: 80, Training loss: 24.14505511522293\n",
      "Epoch: 81, Training loss: 22.821285128593445\n",
      "Epoch: 82, Training loss: 25.371673703193665\n",
      "Epoch: 83, Training loss: 25.105080395936966\n",
      "Epoch: 84, Training loss: 25.15941259264946\n",
      "Epoch: 85, Training loss: 24.505747199058533\n",
      "Epoch: 86, Training loss: 24.722937375307083\n",
      "Epoch: 87, Training loss: 23.663125872612\n",
      "Epoch: 88, Training loss: 25.495909065008163\n",
      "Epoch: 89, Training loss: 24.132906883955002\n",
      "Epoch: 90, Training loss: 25.403906643390656\n",
      "Epoch: 91, Training loss: 23.0529405772686\n",
      "Epoch: 92, Training loss: 23.32257679104805\n",
      "Epoch: 93, Training loss: 25.265519440174103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Training loss: 23.557629495859146\n",
      "Epoch: 95, Training loss: 21.990602269768715\n",
      "Epoch: 96, Training loss: 22.250912874937057\n",
      "Epoch: 97, Training loss: 24.73802226781845\n",
      "Epoch: 98, Training loss: 24.33612272143364\n",
      "Epoch: 99, Training loss: 25.382890731096268\n",
      "Epoch: 100, Training loss: 24.697693526744843\n",
      "Epoch: 101, Training loss: 24.988746911287308\n",
      "Epoch: 102, Training loss: 25.798282384872437\n",
      "Epoch: 103, Training loss: 24.635311782360077\n",
      "Epoch: 104, Training loss: 24.093108594417572\n",
      "Epoch: 105, Training loss: 22.57453763484955\n",
      "Epoch: 106, Training loss: 25.714886784553528\n",
      "Epoch: 107, Training loss: 24.497355550527573\n",
      "Epoch: 108, Training loss: 25.828312546014786\n",
      "Epoch: 109, Training loss: 23.150312453508377\n",
      "Epoch: 110, Training loss: 23.199947327375412\n",
      "Epoch: 111, Training loss: 23.16911953687668\n",
      "Epoch: 112, Training loss: 23.371692836284637\n",
      "Epoch: 113, Training loss: 26.228709936141968\n",
      "Epoch: 114, Training loss: 23.98139598965645\n",
      "Epoch: 115, Training loss: 25.01992192864418\n",
      "Epoch: 116, Training loss: 23.204808801412582\n",
      "Epoch: 117, Training loss: 26.33695012331009\n",
      "Epoch: 118, Training loss: 24.01837921142578\n",
      "Epoch: 119, Training loss: 23.098970532417297\n",
      "Epoch: 120, Training loss: 23.850725203752518\n",
      "Epoch: 121, Training loss: 23.399650275707245\n",
      "Epoch: 122, Training loss: 23.80351209640503\n",
      "Epoch: 123, Training loss: 26.539811313152313\n",
      "Epoch: 124, Training loss: 23.793376982212067\n",
      "Epoch: 125, Training loss: 25.140678018331528\n",
      "Epoch: 126, Training loss: 25.090563893318176\n",
      "Epoch: 127, Training loss: 23.05371606349945\n",
      "Epoch: 128, Training loss: 25.536913633346558\n",
      "Epoch: 129, Training loss: 23.7030867934227\n",
      "Epoch: 130, Training loss: 23.676930367946625\n",
      "Epoch: 131, Training loss: 24.809794932603836\n",
      "Epoch: 132, Training loss: 23.393567711114883\n",
      "Epoch: 133, Training loss: 25.888746082782745\n",
      "Epoch: 134, Training loss: 25.919064790010452\n",
      "Epoch: 135, Training loss: 23.61069428920746\n",
      "Epoch: 136, Training loss: 24.368138939142227\n",
      "Epoch: 137, Training loss: 24.244640052318573\n",
      "Epoch: 138, Training loss: 25.21235191822052\n",
      "Epoch: 139, Training loss: 23.23139750957489\n",
      "Epoch: 140, Training loss: 24.849738359451294\n",
      "Epoch: 141, Training loss: 23.70351952314377\n",
      "Epoch: 142, Training loss: 24.399910777807236\n",
      "Epoch: 143, Training loss: 23.88904970884323\n",
      "Epoch: 144, Training loss: 23.884417980909348\n",
      "Epoch: 145, Training loss: 24.30110976099968\n",
      "Epoch: 146, Training loss: 23.977195769548416\n",
      "Epoch: 147, Training loss: 25.145299971103668\n",
      "Epoch: 148, Training loss: 26.91409057378769\n",
      "Epoch: 149, Training loss: 26.203460037708282\n",
      "Epoch: 150, Training loss: 25.658750772476196\n",
      "Epoch: 151, Training loss: 23.394895762205124\n",
      "Epoch: 152, Training loss: 27.30348426103592\n",
      "Epoch: 153, Training loss: 25.36088466644287\n",
      "Epoch: 154, Training loss: 23.48432394862175\n",
      "Epoch: 155, Training loss: 24.47427624464035\n",
      "Epoch: 156, Training loss: 24.723150849342346\n",
      "Epoch: 157, Training loss: 25.485674679279327\n",
      "Epoch: 158, Training loss: 23.356716603040695\n",
      "Epoch: 159, Training loss: 26.099443316459656\n",
      "Epoch: 160, Training loss: 23.680522859096527\n",
      "Epoch: 161, Training loss: 25.244981169700623\n",
      "Epoch: 162, Training loss: 23.701834619045258\n",
      "Epoch: 163, Training loss: 24.361087799072266\n",
      "Epoch: 164, Training loss: 23.895633190870285\n",
      "Epoch: 165, Training loss: 25.97778108716011\n",
      "Epoch: 166, Training loss: 24.67464989423752\n",
      "Epoch: 167, Training loss: 23.971143692731857\n",
      "Epoch: 168, Training loss: 26.271836161613464\n",
      "Epoch: 169, Training loss: 23.399005472660065\n",
      "Epoch: 170, Training loss: 24.166911363601685\n",
      "Epoch: 171, Training loss: 24.94706591963768\n",
      "Epoch: 172, Training loss: 24.229702949523926\n",
      "Epoch: 173, Training loss: 24.73918968439102\n",
      "Epoch: 174, Training loss: 23.873868107795715\n",
      "Epoch: 175, Training loss: 22.87544697523117\n",
      "Epoch: 176, Training loss: 24.19922026991844\n",
      "Epoch: 177, Training loss: 26.74953505396843\n",
      "Epoch: 178, Training loss: 25.36972799897194\n",
      "Epoch: 179, Training loss: 24.54638060927391\n",
      "Epoch: 180, Training loss: 23.60652393102646\n",
      "Epoch: 181, Training loss: 23.017498075962067\n",
      "Epoch: 182, Training loss: 26.73705354332924\n",
      "Epoch: 183, Training loss: 25.61551943421364\n",
      "Epoch: 184, Training loss: 25.027827322483063\n",
      "Epoch: 185, Training loss: 25.293001979589462\n",
      "Epoch: 186, Training loss: 22.26872479915619\n",
      "Epoch: 187, Training loss: 25.036309838294983\n",
      "Epoch: 188, Training loss: 22.213332802057266\n",
      "Epoch: 189, Training loss: 24.94423171877861\n",
      "Epoch: 190, Training loss: 25.418046355247498\n",
      "Epoch: 191, Training loss: 23.8609561920166\n",
      "Epoch: 192, Training loss: 24.981858134269714\n",
      "Epoch: 193, Training loss: 23.747313857078552\n",
      "Epoch: 194, Training loss: 24.058437705039978\n",
      "Epoch: 195, Training loss: 25.02491268515587\n",
      "Epoch: 196, Training loss: 25.078997492790222\n",
      "Epoch: 197, Training loss: 25.007491290569305\n",
      "Epoch: 198, Training loss: 24.866202741861343\n",
      "Epoch: 199, Training loss: 24.64374029636383\n",
      "Epoch: 200, Training loss: 25.57348096370697\n",
      "Epoch: 201, Training loss: 26.275700867176056\n",
      "Epoch: 202, Training loss: 24.003394097089767\n",
      "Epoch: 203, Training loss: 22.983444273471832\n",
      "Epoch: 204, Training loss: 25.72235345840454\n",
      "Epoch: 205, Training loss: 25.605218440294266\n",
      "Epoch: 206, Training loss: 24.66572278738022\n",
      "Epoch: 207, Training loss: 23.094255596399307\n",
      "Epoch: 208, Training loss: 26.004190385341644\n",
      "Epoch: 209, Training loss: 24.367397412657738\n",
      "Epoch: 210, Training loss: 24.512945234775543\n",
      "Epoch: 211, Training loss: 24.65648478269577\n",
      "Epoch: 212, Training loss: 24.629671305418015\n",
      "Epoch: 213, Training loss: 23.16924422979355\n",
      "Epoch: 214, Training loss: 22.5672168135643\n",
      "Epoch: 215, Training loss: 26.18128901720047\n",
      "Epoch: 216, Training loss: 23.45668315887451\n",
      "Epoch: 217, Training loss: 25.213244944810867\n",
      "Epoch: 218, Training loss: 24.817672312259674\n",
      "Epoch: 219, Training loss: 23.52164500951767\n",
      "Epoch: 220, Training loss: 23.922386407852173\n",
      "Epoch: 221, Training loss: 24.21739536523819\n",
      "Epoch: 222, Training loss: 24.443366050720215\n",
      "Epoch: 223, Training loss: 23.322094082832336\n",
      "Epoch: 224, Training loss: 23.324101954698563\n",
      "Epoch: 225, Training loss: 24.278486222028732\n",
      "Epoch: 226, Training loss: 24.585291028022766\n",
      "Epoch: 227, Training loss: 24.82477766275406\n",
      "Epoch: 228, Training loss: 24.145083248615265\n",
      "Epoch: 229, Training loss: 23.020224690437317\n",
      "Epoch: 230, Training loss: 26.02551817893982\n",
      "Epoch: 231, Training loss: 22.952549815177917\n",
      "Epoch: 232, Training loss: 24.86615538597107\n",
      "Epoch: 233, Training loss: 26.609789490699768\n",
      "Epoch: 234, Training loss: 26.287384629249573\n",
      "Epoch: 235, Training loss: 24.89450141787529\n",
      "Epoch: 236, Training loss: 23.528935313224792\n",
      "Epoch: 237, Training loss: 26.845211625099182\n",
      "Epoch: 238, Training loss: 24.00145134329796\n",
      "Epoch: 239, Training loss: 25.179491877555847\n",
      "Epoch: 240, Training loss: 24.116494804620743\n",
      "Epoch: 241, Training loss: 25.007298350334167\n",
      "Epoch: 242, Training loss: 24.413309812545776\n",
      "Epoch: 243, Training loss: 24.630858838558197\n",
      "Epoch: 244, Training loss: 25.529360502958298\n",
      "Epoch: 245, Training loss: 23.97026987373829\n",
      "Epoch: 246, Training loss: 24.104100048542023\n",
      "Epoch: 247, Training loss: 25.123900443315506\n",
      "Epoch: 248, Training loss: 22.408549308776855\n",
      "Epoch: 249, Training loss: 25.58077648282051\n",
      "Epoch: 250, Training loss: 24.874130845069885\n",
      "Epoch: 251, Training loss: 23.828585624694824\n",
      "Epoch: 252, Training loss: 24.620248794555664\n",
      "Epoch: 253, Training loss: 23.934758841991425\n",
      "Epoch: 254, Training loss: 24.11113005876541\n",
      "Epoch: 255, Training loss: 25.350295513868332\n",
      "Epoch: 256, Training loss: 24.827731430530548\n",
      "Epoch: 257, Training loss: 21.741067349910736\n",
      "Epoch: 258, Training loss: 25.537963703274727\n",
      "Epoch: 259, Training loss: 23.582751482725143\n",
      "Epoch: 260, Training loss: 24.609646439552307\n",
      "Epoch: 261, Training loss: 26.426765471696854\n",
      "Epoch: 262, Training loss: 24.353053033351898\n",
      "Epoch: 263, Training loss: 24.94658488035202\n",
      "Epoch: 264, Training loss: 24.681599587202072\n",
      "Epoch: 265, Training loss: 23.178294986486435\n",
      "Epoch: 266, Training loss: 25.1994491815567\n",
      "Epoch: 267, Training loss: 23.465821593999863\n",
      "Epoch: 268, Training loss: 24.2254855632782\n",
      "Epoch: 269, Training loss: 24.128892481327057\n",
      "Epoch: 270, Training loss: 24.368326723575592\n",
      "Epoch: 271, Training loss: 22.213987320661545\n",
      "Epoch: 272, Training loss: 22.653694212436676\n",
      "Epoch: 273, Training loss: 22.205775052309036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274, Training loss: 23.33619925379753\n",
      "Epoch: 275, Training loss: 24.962097853422165\n",
      "Epoch: 276, Training loss: 22.822865962982178\n",
      "Epoch: 277, Training loss: 25.509509712457657\n",
      "Epoch: 278, Training loss: 21.56855109333992\n",
      "Epoch: 279, Training loss: 24.822332352399826\n",
      "Epoch: 280, Training loss: 21.783277451992035\n",
      "Epoch: 281, Training loss: 23.882313549518585\n",
      "Epoch: 282, Training loss: 24.269938707351685\n",
      "Epoch: 283, Training loss: 24.93085378408432\n",
      "Epoch: 284, Training loss: 24.347811460494995\n",
      "Epoch: 285, Training loss: 23.49036741256714\n",
      "Epoch: 286, Training loss: 25.37663447856903\n",
      "Epoch: 287, Training loss: 22.073930501937866\n",
      "Epoch: 288, Training loss: 26.040938794612885\n",
      "Epoch: 289, Training loss: 26.411601215600967\n",
      "Epoch: 290, Training loss: 23.776374012231827\n",
      "Epoch: 291, Training loss: 24.34777694940567\n",
      "Epoch: 292, Training loss: 23.517744660377502\n",
      "Epoch: 293, Training loss: 23.926787197589874\n",
      "Epoch: 294, Training loss: 24.36300814151764\n",
      "Epoch: 295, Training loss: 23.748432487249374\n",
      "Epoch: 296, Training loss: 26.16073614358902\n",
      "Epoch: 297, Training loss: 24.52164524793625\n",
      "Epoch: 298, Training loss: 23.36274303495884\n",
      "Epoch: 299, Training loss: 23.73668998479843\n",
      "Epoch: 300, Training loss: 23.48069030046463\n",
      "Epoch: 301, Training loss: 23.747113943099976\n",
      "Epoch: 302, Training loss: 25.92676442861557\n",
      "Epoch: 303, Training loss: 23.70417383313179\n",
      "Epoch: 304, Training loss: 24.86900132894516\n",
      "Epoch: 305, Training loss: 23.766051590442657\n",
      "Epoch: 306, Training loss: 26.323059618473053\n",
      "Epoch: 307, Training loss: 22.704383194446564\n",
      "Epoch: 308, Training loss: 24.155291080474854\n",
      "Epoch: 309, Training loss: 24.696716785430908\n",
      "Epoch: 310, Training loss: 24.986708492040634\n",
      "Epoch: 311, Training loss: 25.27154815196991\n",
      "Epoch: 312, Training loss: 27.8081636428833\n",
      "Epoch: 313, Training loss: 25.317074596881866\n",
      "Epoch: 314, Training loss: 24.12516462802887\n",
      "Epoch: 315, Training loss: 25.558303594589233\n",
      "Epoch: 316, Training loss: 25.519689321517944\n",
      "Epoch: 317, Training loss: 25.257597625255585\n",
      "Epoch: 318, Training loss: 24.234696447849274\n",
      "Epoch: 319, Training loss: 23.514887750148773\n",
      "Epoch: 320, Training loss: 23.48756569623947\n",
      "Epoch: 321, Training loss: 22.362881869077682\n",
      "Epoch: 322, Training loss: 22.904966294765472\n",
      "Epoch: 323, Training loss: 25.697806477546692\n",
      "Epoch: 324, Training loss: 22.890538901090622\n",
      "Epoch: 325, Training loss: 22.68156796693802\n",
      "Epoch: 326, Training loss: 22.250022053718567\n",
      "Epoch: 327, Training loss: 24.115889012813568\n",
      "Epoch: 328, Training loss: 25.33308082818985\n",
      "Epoch: 329, Training loss: 23.828481450676918\n",
      "Epoch: 330, Training loss: 23.790054321289062\n",
      "Epoch: 331, Training loss: 24.089837938547134\n",
      "Epoch: 332, Training loss: 27.339409708976746\n",
      "Epoch: 333, Training loss: 23.144187182188034\n",
      "Epoch: 334, Training loss: 21.900898039340973\n",
      "Epoch: 335, Training loss: 24.718450009822845\n",
      "Epoch: 336, Training loss: 26.04537534713745\n",
      "Epoch: 337, Training loss: 23.953852146863937\n",
      "Epoch: 338, Training loss: 24.573238611221313\n",
      "Epoch: 339, Training loss: 25.30126079916954\n",
      "Epoch: 340, Training loss: 24.229258596897125\n",
      "Epoch: 341, Training loss: 22.857594043016434\n",
      "Epoch: 342, Training loss: 22.817767947912216\n",
      "Epoch: 343, Training loss: 23.548998028039932\n",
      "Epoch: 344, Training loss: 24.54578334093094\n",
      "Epoch: 345, Training loss: 26.04947593808174\n",
      "Epoch: 346, Training loss: 23.47191247344017\n",
      "Epoch: 347, Training loss: 24.97825860977173\n",
      "Epoch: 348, Training loss: 26.092675387859344\n",
      "Epoch: 349, Training loss: 24.788874864578247\n",
      "Epoch: 350, Training loss: 23.25211888551712\n",
      "Epoch: 351, Training loss: 24.662085473537445\n",
      "Epoch: 352, Training loss: 23.539365082979202\n",
      "Epoch: 353, Training loss: 25.048123329877853\n",
      "Epoch: 354, Training loss: 24.04314684867859\n",
      "Epoch: 355, Training loss: 24.492349207401276\n",
      "Epoch: 356, Training loss: 23.7955721616745\n",
      "Epoch: 357, Training loss: 25.347515642642975\n",
      "Epoch: 358, Training loss: 25.10310772061348\n",
      "Epoch: 359, Training loss: 24.721505790948868\n",
      "Epoch: 360, Training loss: 23.08605606853962\n",
      "Epoch: 361, Training loss: 25.488062620162964\n",
      "Epoch: 362, Training loss: 24.553897261619568\n",
      "Epoch: 363, Training loss: 22.188423991203308\n",
      "Epoch: 364, Training loss: 24.75686001777649\n",
      "Epoch: 365, Training loss: 23.69785287976265\n",
      "Epoch: 366, Training loss: 24.340552121400833\n",
      "Epoch: 367, Training loss: 24.38156396150589\n",
      "Epoch: 368, Training loss: 23.176664113998413\n",
      "Epoch: 369, Training loss: 24.584901094436646\n",
      "Epoch: 370, Training loss: 23.0378098487854\n",
      "Epoch: 371, Training loss: 24.55021196603775\n",
      "Epoch: 372, Training loss: 24.52506172657013\n",
      "Epoch: 373, Training loss: 25.351135224103928\n",
      "Epoch: 374, Training loss: 22.459377199411392\n",
      "Epoch: 375, Training loss: 20.73497498035431\n",
      "Epoch: 376, Training loss: 26.464918613433838\n",
      "Epoch: 377, Training loss: 26.58435833454132\n",
      "Epoch: 378, Training loss: 23.995357900857925\n",
      "Epoch: 379, Training loss: 23.241324484348297\n",
      "Epoch: 380, Training loss: 22.748024940490723\n",
      "Epoch: 381, Training loss: 25.504054725170135\n",
      "Epoch: 382, Training loss: 25.935560941696167\n",
      "Epoch: 383, Training loss: 24.715526819229126\n",
      "Epoch: 384, Training loss: 23.745482981204987\n",
      "Epoch: 385, Training loss: 22.15500921010971\n",
      "Epoch: 386, Training loss: 25.44347769021988\n",
      "Epoch: 387, Training loss: 26.26676833629608\n",
      "Epoch: 388, Training loss: 25.01314252614975\n",
      "Epoch: 389, Training loss: 24.02409142255783\n",
      "Epoch: 390, Training loss: 23.370610266923904\n",
      "Epoch: 391, Training loss: 25.473114550113678\n",
      "Epoch: 392, Training loss: 22.94819349050522\n",
      "Epoch: 393, Training loss: 26.223314464092255\n",
      "Epoch: 394, Training loss: 25.743962317705154\n",
      "Epoch: 395, Training loss: 26.591066777706146\n",
      "Epoch: 396, Training loss: 23.63365861773491\n",
      "Epoch: 397, Training loss: 26.622211396694183\n",
      "Epoch: 398, Training loss: 25.13484674692154\n",
      "Epoch: 399, Training loss: 26.808943569660187\n",
      "Epoch: 400, Training loss: 23.55148881673813\n",
      "Epoch: 401, Training loss: 23.31774801015854\n",
      "Epoch: 402, Training loss: 23.13762789964676\n",
      "Epoch: 403, Training loss: 25.119950830936432\n",
      "Epoch: 404, Training loss: 24.710103422403336\n",
      "Epoch: 405, Training loss: 25.8114912211895\n",
      "Epoch: 406, Training loss: 26.239416033029556\n",
      "Epoch: 407, Training loss: 23.004086911678314\n",
      "Epoch: 408, Training loss: 23.846214175224304\n",
      "Epoch: 409, Training loss: 24.53670382499695\n",
      "Epoch: 410, Training loss: 24.47602903842926\n",
      "Epoch: 411, Training loss: 24.212018847465515\n",
      "Epoch: 412, Training loss: 23.01430344581604\n",
      "Epoch: 413, Training loss: 24.478455185890198\n",
      "Epoch: 414, Training loss: 24.17778167128563\n",
      "Epoch: 415, Training loss: 24.511923730373383\n",
      "Epoch: 416, Training loss: 23.87956142425537\n",
      "Epoch: 417, Training loss: 26.763655424118042\n",
      "Epoch: 418, Training loss: 24.272675037384033\n",
      "Epoch: 419, Training loss: 24.642230957746506\n",
      "Epoch: 420, Training loss: 23.114749789237976\n",
      "Epoch: 421, Training loss: 24.20532065629959\n",
      "Epoch: 422, Training loss: 23.287244707345963\n",
      "Epoch: 423, Training loss: 23.36898848414421\n",
      "Epoch: 424, Training loss: 25.257263004779816\n",
      "Epoch: 425, Training loss: 25.175619184970856\n",
      "Epoch: 426, Training loss: 24.97147038578987\n",
      "Epoch: 427, Training loss: 22.642673432826996\n",
      "Epoch: 428, Training loss: 25.831620067358017\n",
      "Epoch: 429, Training loss: 24.617909520864487\n",
      "Epoch: 430, Training loss: 23.159050077199936\n",
      "Epoch: 431, Training loss: 24.09701493382454\n",
      "Epoch: 432, Training loss: 25.73654806613922\n",
      "Epoch: 433, Training loss: 24.680350095033646\n",
      "Epoch: 434, Training loss: 23.68579539656639\n",
      "Epoch: 435, Training loss: 25.34771355986595\n",
      "Epoch: 436, Training loss: 26.610113084316254\n",
      "Epoch: 437, Training loss: 25.22707587480545\n",
      "Epoch: 438, Training loss: 24.740133702754974\n",
      "Epoch: 439, Training loss: 24.730073630809784\n",
      "Epoch: 440, Training loss: 24.01927974820137\n",
      "Epoch: 441, Training loss: 22.760744214057922\n",
      "Epoch: 442, Training loss: 24.71292120218277\n",
      "Epoch: 443, Training loss: 22.3613363802433\n",
      "Epoch: 444, Training loss: 25.03956490755081\n",
      "Epoch: 445, Training loss: 24.581727743148804\n",
      "Epoch: 446, Training loss: 24.648797124624252\n",
      "Epoch: 447, Training loss: 25.210380911827087\n",
      "Epoch: 448, Training loss: 26.057476609945297\n",
      "Epoch: 449, Training loss: 22.330744832754135\n",
      "Epoch: 450, Training loss: 23.08671137690544\n",
      "Epoch: 451, Training loss: 22.346933752298355\n",
      "Epoch: 452, Training loss: 26.324455857276917\n",
      "Epoch: 453, Training loss: 23.68375661969185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 454, Training loss: 24.45513144135475\n",
      "Epoch: 455, Training loss: 24.823284775018692\n",
      "Epoch: 456, Training loss: 24.735634565353394\n",
      "Epoch: 457, Training loss: 23.46742931008339\n",
      "Epoch: 458, Training loss: 24.61489972472191\n",
      "Epoch: 459, Training loss: 24.094364523887634\n",
      "Epoch: 460, Training loss: 25.69282928109169\n",
      "Epoch: 461, Training loss: 24.394845575094223\n",
      "Epoch: 462, Training loss: 23.15839746594429\n",
      "Epoch: 463, Training loss: 23.52193759381771\n",
      "Epoch: 464, Training loss: 24.79129946231842\n",
      "Epoch: 465, Training loss: 23.875459998846054\n",
      "Epoch: 466, Training loss: 23.63926190137863\n",
      "Epoch: 467, Training loss: 25.428847789764404\n",
      "Epoch: 468, Training loss: 25.343254655599594\n",
      "Epoch: 469, Training loss: 24.928937673568726\n",
      "Epoch: 470, Training loss: 25.486048489809036\n",
      "Epoch: 471, Training loss: 22.34949430823326\n",
      "Epoch: 472, Training loss: 25.576858073472977\n",
      "Epoch: 473, Training loss: 23.572604179382324\n",
      "Epoch: 474, Training loss: 27.523498058319092\n",
      "Epoch: 475, Training loss: 23.25532692670822\n",
      "Epoch: 476, Training loss: 24.524185001850128\n",
      "Epoch: 477, Training loss: 23.08289459347725\n",
      "Epoch: 478, Training loss: 24.57789522409439\n",
      "Epoch: 479, Training loss: 23.63417676091194\n",
      "Epoch: 480, Training loss: 25.535523295402527\n",
      "Epoch: 481, Training loss: 24.41156440973282\n",
      "Epoch: 482, Training loss: 24.14287930727005\n",
      "Epoch: 483, Training loss: 25.717754781246185\n",
      "Epoch: 484, Training loss: 25.54489094018936\n",
      "Epoch: 485, Training loss: 22.172115117311478\n",
      "Epoch: 486, Training loss: 24.39375615119934\n",
      "Epoch: 487, Training loss: 25.64710345864296\n",
      "Epoch: 488, Training loss: 23.390607714653015\n",
      "Epoch: 489, Training loss: 22.100877106189728\n",
      "Epoch: 490, Training loss: 24.642534136772156\n",
      "Epoch: 491, Training loss: 25.680539190769196\n",
      "Epoch: 492, Training loss: 26.72558018565178\n",
      "Epoch: 493, Training loss: 24.649222493171692\n",
      "Epoch: 494, Training loss: 25.029386162757874\n",
      "Epoch: 495, Training loss: 24.55808237195015\n",
      "Epoch: 496, Training loss: 23.495334774255753\n",
      "Epoch: 497, Training loss: 26.013581812381744\n",
      "Epoch: 498, Training loss: 23.202867448329926\n",
      "Epoch: 499, Training loss: 24.009986460208893\n",
      "Epoch: 500, Training loss: 24.88210356235504\n",
      "Epoch: 501, Training loss: 23.50910595059395\n",
      "Epoch: 502, Training loss: 25.070782721042633\n",
      "Epoch: 503, Training loss: 24.239042043685913\n",
      "Epoch: 504, Training loss: 24.575344175100327\n",
      "Epoch: 505, Training loss: 23.550717800855637\n",
      "Epoch: 506, Training loss: 22.209032148122787\n",
      "Epoch: 507, Training loss: 23.17387366294861\n",
      "Epoch: 508, Training loss: 25.90293860435486\n",
      "Epoch: 509, Training loss: 22.626839846372604\n",
      "Epoch: 510, Training loss: 22.311230182647705\n",
      "Epoch: 511, Training loss: 25.060159653425217\n",
      "Epoch: 512, Training loss: 23.61974999308586\n",
      "Epoch: 513, Training loss: 25.197114050388336\n",
      "Epoch: 514, Training loss: 23.921068787574768\n",
      "Epoch: 515, Training loss: 25.70494544506073\n",
      "Epoch: 516, Training loss: 25.01009052991867\n",
      "Epoch: 517, Training loss: 25.596104085445404\n",
      "Epoch: 518, Training loss: 25.07778251171112\n",
      "Epoch: 519, Training loss: 25.822411328554153\n",
      "Epoch: 520, Training loss: 25.063483119010925\n",
      "Epoch: 521, Training loss: 25.97500839829445\n",
      "Epoch: 522, Training loss: 22.05491292476654\n",
      "Epoch: 523, Training loss: 24.56933969259262\n",
      "Epoch: 524, Training loss: 24.28761050105095\n",
      "Epoch: 525, Training loss: 24.48902878165245\n",
      "Epoch: 526, Training loss: 23.02552843093872\n",
      "Epoch: 527, Training loss: 25.80825662612915\n",
      "Epoch: 528, Training loss: 24.6938556432724\n",
      "Epoch: 529, Training loss: 24.947412073612213\n",
      "Epoch: 530, Training loss: 24.67680299282074\n",
      "Epoch: 531, Training loss: 23.913359701633453\n",
      "Epoch: 532, Training loss: 26.220726758241653\n",
      "Epoch: 533, Training loss: 23.131177335977554\n",
      "Epoch: 534, Training loss: 25.397007703781128\n",
      "Epoch: 535, Training loss: 24.289568722248077\n",
      "Epoch: 536, Training loss: 25.990284204483032\n",
      "Epoch: 537, Training loss: 26.030845493078232\n",
      "Epoch: 538, Training loss: 24.18397119641304\n",
      "Epoch: 539, Training loss: 23.961826503276825\n",
      "Epoch: 540, Training loss: 25.583462357521057\n",
      "Epoch: 541, Training loss: 22.730891287326813\n",
      "Epoch: 542, Training loss: 23.43919825553894\n",
      "Epoch: 543, Training loss: 24.936027854681015\n",
      "Epoch: 544, Training loss: 23.664665043354034\n",
      "Epoch: 545, Training loss: 25.397719383239746\n",
      "Epoch: 546, Training loss: 24.32051193714142\n",
      "Epoch: 547, Training loss: 23.678495705127716\n",
      "Epoch: 548, Training loss: 23.289041996002197\n",
      "Epoch: 549, Training loss: 22.255112677812576\n",
      "Epoch: 550, Training loss: 23.16633227467537\n",
      "Epoch: 551, Training loss: 25.840141594409943\n",
      "Epoch: 552, Training loss: 23.474897027015686\n",
      "Epoch: 553, Training loss: 23.07590827345848\n",
      "Epoch: 554, Training loss: 23.597653716802597\n",
      "Epoch: 555, Training loss: 23.772197395563126\n",
      "Epoch: 556, Training loss: 24.780590295791626\n",
      "Epoch: 557, Training loss: 24.38449802994728\n",
      "Epoch: 558, Training loss: 22.686069786548615\n",
      "Epoch: 559, Training loss: 24.87244513630867\n",
      "Epoch: 560, Training loss: 25.23641112446785\n",
      "Epoch: 561, Training loss: 24.69741266965866\n",
      "Epoch: 562, Training loss: 23.497739255428314\n",
      "Epoch: 563, Training loss: 23.729193150997162\n",
      "Epoch: 564, Training loss: 25.756959915161133\n",
      "Epoch: 565, Training loss: 25.950861871242523\n",
      "Epoch: 566, Training loss: 24.192036747932434\n",
      "Epoch: 567, Training loss: 24.504523634910583\n",
      "Epoch: 568, Training loss: 23.872418642044067\n",
      "Epoch: 569, Training loss: 23.198321878910065\n",
      "Epoch: 570, Training loss: 23.45292365550995\n",
      "Epoch: 571, Training loss: 24.093669593334198\n",
      "Epoch: 572, Training loss: 24.828800201416016\n",
      "Epoch: 573, Training loss: 24.77688765525818\n",
      "Epoch: 574, Training loss: 23.767936408519745\n",
      "Epoch: 575, Training loss: 25.646531105041504\n",
      "Epoch: 576, Training loss: 24.38987022638321\n",
      "Epoch: 577, Training loss: 25.968417704105377\n",
      "Epoch: 578, Training loss: 24.150126665830612\n",
      "Epoch: 579, Training loss: 22.72441405057907\n",
      "Epoch: 580, Training loss: 23.39586842060089\n",
      "Epoch: 581, Training loss: 23.232504338026047\n",
      "Epoch: 582, Training loss: 25.839638233184814\n",
      "Epoch: 583, Training loss: 22.91488042473793\n",
      "Epoch: 584, Training loss: 25.133354544639587\n",
      "Epoch: 585, Training loss: 24.39491856098175\n",
      "Epoch: 586, Training loss: 23.768769398331642\n",
      "Epoch: 587, Training loss: 24.10735708475113\n",
      "Epoch: 588, Training loss: 24.651401609182358\n",
      "Epoch: 589, Training loss: 24.971718728542328\n",
      "Epoch: 590, Training loss: 21.64398768544197\n",
      "Epoch: 591, Training loss: 24.96406990289688\n",
      "Epoch: 592, Training loss: 23.4615358710289\n",
      "Epoch: 593, Training loss: 22.906061351299286\n",
      "Epoch: 594, Training loss: 24.779883444309235\n",
      "Epoch: 595, Training loss: 24.66706210374832\n",
      "Epoch: 596, Training loss: 24.897137880325317\n",
      "Epoch: 597, Training loss: 23.601861774921417\n",
      "Epoch: 598, Training loss: 25.309930682182312\n",
      "Epoch: 599, Training loss: 22.8778854906559\n",
      "Epoch: 600, Training loss: 26.115636229515076\n",
      "Epoch: 601, Training loss: 24.37406712770462\n",
      "Epoch: 602, Training loss: 24.691752672195435\n",
      "Epoch: 603, Training loss: 25.02047085762024\n",
      "Epoch: 604, Training loss: 24.79563993215561\n",
      "Epoch: 605, Training loss: 25.214554399251938\n",
      "Epoch: 606, Training loss: 23.646857380867004\n",
      "Epoch: 607, Training loss: 23.96895307302475\n",
      "Epoch: 608, Training loss: 25.543468177318573\n",
      "Epoch: 609, Training loss: 23.43696489930153\n",
      "Epoch: 610, Training loss: 24.154147028923035\n",
      "Epoch: 611, Training loss: 24.248029708862305\n",
      "Epoch: 612, Training loss: 23.403511732816696\n",
      "Epoch: 613, Training loss: 23.64877137541771\n",
      "Epoch: 614, Training loss: 25.38517665863037\n",
      "Epoch: 615, Training loss: 25.38932329416275\n",
      "Epoch: 616, Training loss: 24.963427543640137\n",
      "Epoch: 617, Training loss: 24.133065938949585\n",
      "Epoch: 618, Training loss: 24.27943354845047\n",
      "Epoch: 619, Training loss: 22.982861399650574\n",
      "Epoch: 620, Training loss: 25.166040301322937\n",
      "Epoch: 621, Training loss: 24.026698261499405\n",
      "Epoch: 622, Training loss: 27.537641495466232\n",
      "Epoch: 623, Training loss: 23.9499209523201\n",
      "Epoch: 624, Training loss: 23.527147889137268\n",
      "Epoch: 625, Training loss: 22.651595264673233\n",
      "Epoch: 626, Training loss: 24.200937777757645\n",
      "Epoch: 627, Training loss: 23.72555986046791\n",
      "Epoch: 628, Training loss: 24.97964835166931\n",
      "Epoch: 629, Training loss: 25.334136307239532\n",
      "Epoch: 630, Training loss: 24.457007378339767\n",
      "Epoch: 631, Training loss: 24.95952731370926\n",
      "Epoch: 632, Training loss: 24.637168884277344\n",
      "Epoch: 633, Training loss: 24.38191184401512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 634, Training loss: 23.351750314235687\n",
      "Epoch: 635, Training loss: 24.09518301486969\n",
      "Epoch: 636, Training loss: 22.89094188809395\n",
      "Epoch: 637, Training loss: 24.75745663046837\n",
      "Epoch: 638, Training loss: 23.495872855186462\n",
      "Epoch: 639, Training loss: 24.532780319452286\n",
      "Epoch: 640, Training loss: 24.355440974235535\n",
      "Epoch: 641, Training loss: 25.30368483066559\n",
      "Epoch: 642, Training loss: 24.520663559436798\n",
      "Epoch: 643, Training loss: 23.826566442847252\n",
      "Epoch: 644, Training loss: 25.281260132789612\n",
      "Epoch: 645, Training loss: 24.693116664886475\n",
      "Epoch: 646, Training loss: 24.613766819238663\n",
      "Epoch: 647, Training loss: 24.785566449165344\n",
      "Epoch: 648, Training loss: 26.04666304588318\n",
      "Epoch: 649, Training loss: 26.206894993782043\n",
      "Epoch: 650, Training loss: 22.168488919734955\n",
      "Epoch: 651, Training loss: 23.925707578659058\n",
      "Epoch: 652, Training loss: 23.750818073749542\n",
      "Epoch: 653, Training loss: 24.931244611740112\n",
      "Epoch: 654, Training loss: 23.847433179616928\n",
      "Epoch: 655, Training loss: 25.123892724514008\n",
      "Epoch: 656, Training loss: 25.676803529262543\n",
      "Epoch: 657, Training loss: 23.97653079032898\n",
      "Epoch: 658, Training loss: 24.92310693860054\n",
      "Epoch: 659, Training loss: 23.77114224433899\n",
      "Epoch: 660, Training loss: 24.110522359609604\n",
      "Epoch: 661, Training loss: 23.110228657722473\n",
      "Epoch: 662, Training loss: 23.401494920253754\n",
      "Epoch: 663, Training loss: 23.688473850488663\n",
      "Epoch: 664, Training loss: 24.798738956451416\n",
      "Epoch: 665, Training loss: 26.18963199853897\n",
      "Epoch: 666, Training loss: 23.569825381040573\n",
      "Epoch: 667, Training loss: 23.887365996837616\n",
      "Epoch: 668, Training loss: 25.151589393615723\n",
      "Epoch: 669, Training loss: 25.307622969150543\n",
      "Epoch: 670, Training loss: 23.45507922768593\n",
      "Epoch: 671, Training loss: 24.032623767852783\n",
      "Epoch: 672, Training loss: 24.734489768743515\n",
      "Epoch: 673, Training loss: 24.32837623357773\n",
      "Epoch: 674, Training loss: 23.56250688433647\n",
      "Epoch: 675, Training loss: 24.80099669098854\n",
      "Epoch: 676, Training loss: 25.033376425504684\n",
      "Epoch: 677, Training loss: 24.982196778059006\n",
      "Epoch: 678, Training loss: 23.609698116779327\n",
      "Epoch: 679, Training loss: 24.07890883088112\n",
      "Epoch: 680, Training loss: 22.879921078681946\n",
      "Epoch: 681, Training loss: 23.419499456882477\n",
      "Epoch: 682, Training loss: 23.60224014520645\n",
      "Epoch: 683, Training loss: 24.724980145692825\n",
      "Epoch: 684, Training loss: 24.099178791046143\n",
      "Epoch: 685, Training loss: 24.750122010707855\n",
      "Epoch: 686, Training loss: 24.192255213856697\n",
      "Epoch: 687, Training loss: 23.03500860929489\n",
      "Epoch: 688, Training loss: 23.995140463113785\n",
      "Epoch: 689, Training loss: 25.73153644800186\n",
      "Epoch: 690, Training loss: 23.580839455127716\n",
      "Epoch: 691, Training loss: 24.07458120584488\n",
      "Epoch: 692, Training loss: 25.41439774632454\n",
      "Epoch: 693, Training loss: 22.024802178144455\n",
      "Epoch: 694, Training loss: 24.738180309534073\n",
      "Epoch: 695, Training loss: 26.115901052951813\n",
      "Epoch: 696, Training loss: 24.42220151424408\n",
      "Epoch: 697, Training loss: 24.734027594327927\n",
      "Epoch: 698, Training loss: 25.101803302764893\n",
      "Epoch: 699, Training loss: 23.68246829509735\n",
      "Epoch: 700, Training loss: 24.41293901205063\n",
      "Epoch: 701, Training loss: 24.421580940485\n",
      "Epoch: 702, Training loss: 26.49578469991684\n",
      "Epoch: 703, Training loss: 23.087626606225967\n",
      "Epoch: 704, Training loss: 23.592548191547394\n",
      "Epoch: 705, Training loss: 24.910646855831146\n",
      "Epoch: 706, Training loss: 22.237462103366852\n",
      "Epoch: 707, Training loss: 23.850114166736603\n",
      "Epoch: 708, Training loss: 23.64010778069496\n",
      "Epoch: 709, Training loss: 25.382053464651108\n",
      "Epoch: 710, Training loss: 25.251838862895966\n",
      "Epoch: 711, Training loss: 23.752378225326538\n",
      "Epoch: 712, Training loss: 24.365926444530487\n",
      "Epoch: 713, Training loss: 22.98131713271141\n",
      "Epoch: 714, Training loss: 24.565472334623337\n",
      "Epoch: 715, Training loss: 23.82789933681488\n",
      "Epoch: 716, Training loss: 23.22641271352768\n",
      "Epoch: 717, Training loss: 24.788906753063202\n",
      "Epoch: 718, Training loss: 22.77223739027977\n",
      "Epoch: 719, Training loss: 24.068773359060287\n",
      "Epoch: 720, Training loss: 22.96216642856598\n",
      "Epoch: 721, Training loss: 23.44654357433319\n",
      "Epoch: 722, Training loss: 23.760218501091003\n",
      "Epoch: 723, Training loss: 24.29493737220764\n",
      "Epoch: 724, Training loss: 24.835786908864975\n",
      "Epoch: 725, Training loss: 24.94017055630684\n",
      "Epoch: 726, Training loss: 24.562321931123734\n",
      "Epoch: 727, Training loss: 24.41929429769516\n",
      "Epoch: 728, Training loss: 22.911049336194992\n",
      "Epoch: 729, Training loss: 24.17731222510338\n",
      "Epoch: 730, Training loss: 24.927067697048187\n",
      "Epoch: 731, Training loss: 23.065578937530518\n",
      "Epoch: 732, Training loss: 26.99570095539093\n",
      "Epoch: 733, Training loss: 24.944490045309067\n",
      "Epoch: 734, Training loss: 23.99954468011856\n",
      "Epoch: 735, Training loss: 26.08893796801567\n",
      "Epoch: 736, Training loss: 24.34448978304863\n",
      "Epoch: 737, Training loss: 24.397209256887436\n",
      "Epoch: 738, Training loss: 24.101533889770508\n",
      "Epoch: 739, Training loss: 26.07988953590393\n",
      "Epoch: 740, Training loss: 24.565226435661316\n",
      "Epoch: 741, Training loss: 25.73396012187004\n",
      "Epoch: 742, Training loss: 24.38516104221344\n",
      "Epoch: 743, Training loss: 24.162732481956482\n",
      "Epoch: 744, Training loss: 24.111439764499664\n",
      "Epoch: 745, Training loss: 24.474243581295013\n",
      "Epoch: 746, Training loss: 26.565233379602432\n",
      "Epoch: 747, Training loss: 24.348781883716583\n",
      "Epoch: 748, Training loss: 25.452726244926453\n",
      "Epoch: 749, Training loss: 25.022224068641663\n",
      "Epoch: 750, Training loss: 23.091800153255463\n",
      "Epoch: 751, Training loss: 22.443151235580444\n",
      "Epoch: 752, Training loss: 22.53667062520981\n",
      "Epoch: 753, Training loss: 24.092811107635498\n",
      "Epoch: 754, Training loss: 25.294473260641098\n",
      "Epoch: 755, Training loss: 24.358702421188354\n",
      "Epoch: 756, Training loss: 23.001339823007584\n",
      "Epoch: 757, Training loss: 25.61599040031433\n",
      "Epoch: 758, Training loss: 21.731193333864212\n",
      "Epoch: 759, Training loss: 23.742622405290604\n",
      "Epoch: 760, Training loss: 25.09467703104019\n",
      "Epoch: 761, Training loss: 24.533566057682037\n",
      "Epoch: 762, Training loss: 24.46312952041626\n",
      "Epoch: 763, Training loss: 24.328747361898422\n",
      "Epoch: 764, Training loss: 25.0300235748291\n",
      "Epoch: 765, Training loss: 23.57732781767845\n",
      "Epoch: 766, Training loss: 25.520607858896255\n",
      "Epoch: 767, Training loss: 24.844829857349396\n",
      "Epoch: 768, Training loss: 25.867083817720413\n",
      "Epoch: 769, Training loss: 23.421287953853607\n",
      "Epoch: 770, Training loss: 26.111944496631622\n",
      "Epoch: 771, Training loss: 22.682209819555283\n",
      "Epoch: 772, Training loss: 24.48210507631302\n",
      "Epoch: 773, Training loss: 23.024681836366653\n",
      "Epoch: 774, Training loss: 23.70142924785614\n",
      "Epoch: 775, Training loss: 24.635736852884293\n",
      "Epoch: 776, Training loss: 24.7543785572052\n",
      "Epoch: 777, Training loss: 22.204811215400696\n",
      "Epoch: 778, Training loss: 24.786694049835205\n",
      "Epoch: 779, Training loss: 25.707729995250702\n",
      "Epoch: 780, Training loss: 24.847146570682526\n",
      "Epoch: 781, Training loss: 24.14049881696701\n",
      "Epoch: 782, Training loss: 24.58694377541542\n",
      "Epoch: 783, Training loss: 26.63922879099846\n",
      "Epoch: 784, Training loss: 25.056145191192627\n",
      "Epoch: 785, Training loss: 24.244562596082687\n",
      "Epoch: 786, Training loss: 25.26444637775421\n",
      "Epoch: 787, Training loss: 25.30432838201523\n",
      "Epoch: 788, Training loss: 23.922550708055496\n",
      "Epoch: 789, Training loss: 23.603021442890167\n",
      "Epoch: 790, Training loss: 25.642820835113525\n",
      "Epoch: 791, Training loss: 22.698546051979065\n",
      "Epoch: 792, Training loss: 23.217124223709106\n",
      "Epoch: 793, Training loss: 24.066612154245377\n",
      "Epoch: 794, Training loss: 24.061855614185333\n",
      "Epoch: 795, Training loss: 22.983057022094727\n",
      "Epoch: 796, Training loss: 24.123815894126892\n",
      "Epoch: 797, Training loss: 24.2900350689888\n",
      "Epoch: 798, Training loss: 25.95350342988968\n",
      "Epoch: 799, Training loss: 26.01527175307274\n",
      "Epoch: 800, Training loss: 24.602321326732635\n",
      "Epoch: 801, Training loss: 22.109073489904404\n",
      "Epoch: 802, Training loss: 25.372388035058975\n",
      "Epoch: 803, Training loss: 23.059065222740173\n",
      "Epoch: 804, Training loss: 24.039650976657867\n",
      "Epoch: 805, Training loss: 24.425043404102325\n",
      "Epoch: 806, Training loss: 23.48065459728241\n",
      "Epoch: 807, Training loss: 23.70215979218483\n",
      "Epoch: 808, Training loss: 24.47495073080063\n",
      "Epoch: 809, Training loss: 23.423646956682205\n",
      "Epoch: 810, Training loss: 24.39945724606514\n",
      "Epoch: 811, Training loss: 22.827269792556763\n",
      "Epoch: 812, Training loss: 24.877229750156403\n",
      "Epoch: 813, Training loss: 24.933812201023102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 814, Training loss: 24.85101479291916\n",
      "Epoch: 815, Training loss: 24.305821180343628\n",
      "Epoch: 816, Training loss: 22.5457044839859\n",
      "Epoch: 817, Training loss: 24.76820480823517\n",
      "Epoch: 818, Training loss: 23.7043898999691\n",
      "Epoch: 819, Training loss: 23.66207128763199\n",
      "Epoch: 820, Training loss: 24.28065174818039\n",
      "Epoch: 821, Training loss: 23.596241146326065\n",
      "Epoch: 822, Training loss: 24.754083037376404\n",
      "Epoch: 823, Training loss: 24.39766138792038\n",
      "Epoch: 824, Training loss: 25.35897320508957\n",
      "Epoch: 825, Training loss: 26.575993418693542\n",
      "Epoch: 826, Training loss: 26.16848400235176\n",
      "Epoch: 827, Training loss: 24.776134967803955\n",
      "Epoch: 828, Training loss: 22.82915511727333\n",
      "Epoch: 829, Training loss: 25.083335429430008\n",
      "Epoch: 830, Training loss: 24.69001331925392\n",
      "Epoch: 831, Training loss: 25.0909606218338\n",
      "Epoch: 832, Training loss: 23.342412292957306\n",
      "Epoch: 833, Training loss: 23.767358899116516\n",
      "Epoch: 834, Training loss: 24.24027806520462\n",
      "Epoch: 835, Training loss: 25.78055912256241\n",
      "Epoch: 836, Training loss: 26.617821395397186\n",
      "Epoch: 837, Training loss: 23.86644157767296\n",
      "Epoch: 838, Training loss: 23.736348092556\n",
      "Epoch: 839, Training loss: 25.014709949493408\n",
      "Epoch: 840, Training loss: 22.14319211244583\n",
      "Epoch: 841, Training loss: 21.014920994639397\n",
      "Epoch: 842, Training loss: 25.904124706983566\n",
      "Epoch: 843, Training loss: 24.994555115699768\n",
      "Epoch: 844, Training loss: 23.91463354229927\n",
      "Epoch: 845, Training loss: 24.516932427883148\n",
      "Epoch: 846, Training loss: 25.401220738887787\n",
      "Epoch: 847, Training loss: 25.510470747947693\n",
      "Epoch: 848, Training loss: 24.478936195373535\n",
      "Epoch: 849, Training loss: 25.603061258792877\n",
      "Epoch: 850, Training loss: 23.48362386226654\n",
      "Epoch: 851, Training loss: 23.59591919183731\n",
      "Epoch: 852, Training loss: 24.818154633045197\n",
      "Epoch: 853, Training loss: 25.17223733663559\n",
      "Epoch: 854, Training loss: 26.188792407512665\n",
      "Epoch: 855, Training loss: 24.40682762861252\n",
      "Epoch: 856, Training loss: 25.807166785001755\n",
      "Epoch: 857, Training loss: 26.294177621603012\n",
      "Epoch: 858, Training loss: 24.52555900812149\n",
      "Epoch: 859, Training loss: 24.220374166965485\n",
      "Epoch: 860, Training loss: 25.84401848912239\n",
      "Epoch: 861, Training loss: 24.387036830186844\n",
      "Epoch: 862, Training loss: 23.592773735523224\n",
      "Epoch: 863, Training loss: 22.036146879196167\n",
      "Epoch: 864, Training loss: 23.494469612836838\n",
      "Epoch: 865, Training loss: 24.080004453659058\n",
      "Epoch: 866, Training loss: 23.761238038539886\n",
      "Epoch: 867, Training loss: 23.1365387737751\n",
      "Epoch: 868, Training loss: 24.941391497850418\n",
      "Epoch: 869, Training loss: 26.13096296787262\n",
      "Epoch: 870, Training loss: 23.883083194494247\n",
      "Epoch: 871, Training loss: 24.635833859443665\n",
      "Epoch: 872, Training loss: 24.1595598757267\n",
      "Epoch: 873, Training loss: 25.257852256298065\n",
      "Epoch: 874, Training loss: 23.614983469247818\n",
      "Epoch: 875, Training loss: 24.35306689143181\n",
      "Epoch: 876, Training loss: 25.96120995283127\n",
      "Epoch: 877, Training loss: 25.064740240573883\n",
      "Epoch: 878, Training loss: 22.991422653198242\n",
      "Epoch: 879, Training loss: 23.98267674446106\n",
      "Epoch: 880, Training loss: 24.083398789167404\n",
      "Epoch: 881, Training loss: 22.52212055027485\n",
      "Epoch: 882, Training loss: 24.43717148900032\n",
      "Epoch: 883, Training loss: 23.930715531110764\n",
      "Epoch: 884, Training loss: 24.858424365520477\n",
      "Epoch: 885, Training loss: 23.011208176612854\n",
      "Epoch: 886, Training loss: 25.931627839803696\n",
      "Epoch: 887, Training loss: 25.389505624771118\n",
      "Epoch: 888, Training loss: 23.956870943307877\n",
      "Epoch: 889, Training loss: 24.766302049160004\n",
      "Epoch: 890, Training loss: 24.63112524151802\n",
      "Epoch: 891, Training loss: 23.41442894935608\n",
      "Epoch: 892, Training loss: 26.4712233543396\n",
      "Epoch: 893, Training loss: 23.124918282032013\n",
      "Epoch: 894, Training loss: 22.730584114789963\n",
      "Epoch: 895, Training loss: 23.86193335056305\n",
      "Epoch: 896, Training loss: 24.406962871551514\n",
      "Epoch: 897, Training loss: 26.287448585033417\n",
      "Epoch: 898, Training loss: 24.534641057252884\n",
      "Epoch: 899, Training loss: 23.51929098367691\n",
      "Epoch: 900, Training loss: 23.890702784061432\n",
      "Epoch: 901, Training loss: 24.74431464076042\n",
      "Epoch: 902, Training loss: 23.94551983475685\n",
      "Epoch: 903, Training loss: 22.806820899248123\n",
      "Epoch: 904, Training loss: 26.0114067196846\n",
      "Epoch: 905, Training loss: 24.177704572677612\n",
      "Epoch: 906, Training loss: 24.626009345054626\n",
      "Epoch: 907, Training loss: 24.144272208213806\n",
      "Epoch: 908, Training loss: 24.06416293978691\n",
      "Epoch: 909, Training loss: 22.80117490887642\n",
      "Epoch: 910, Training loss: 23.764692068099976\n",
      "Epoch: 911, Training loss: 24.388366639614105\n",
      "Epoch: 912, Training loss: 23.66826358437538\n",
      "Epoch: 913, Training loss: 24.762071639299393\n",
      "Epoch: 914, Training loss: 23.278118640184402\n",
      "Epoch: 915, Training loss: 23.15519008040428\n",
      "Epoch: 916, Training loss: 23.65237009525299\n",
      "Epoch: 917, Training loss: 23.87106764316559\n",
      "Epoch: 918, Training loss: 25.226495534181595\n",
      "Epoch: 919, Training loss: 21.647231727838516\n",
      "Epoch: 920, Training loss: 23.976825147867203\n",
      "Epoch: 921, Training loss: 24.150170028209686\n",
      "Epoch: 922, Training loss: 24.424938678741455\n",
      "Epoch: 923, Training loss: 23.931852281093597\n",
      "Epoch: 924, Training loss: 23.295063316822052\n",
      "Epoch: 925, Training loss: 24.17234429717064\n",
      "Epoch: 926, Training loss: 23.862317502498627\n",
      "Epoch: 927, Training loss: 23.70398712158203\n",
      "Epoch: 928, Training loss: 23.84282198548317\n",
      "Epoch: 929, Training loss: 23.367930948734283\n",
      "Epoch: 930, Training loss: 22.165340304374695\n",
      "Epoch: 931, Training loss: 21.678521424531937\n",
      "Epoch: 932, Training loss: 25.881173253059387\n",
      "Epoch: 933, Training loss: 24.618769645690918\n",
      "Epoch: 934, Training loss: 25.09127253293991\n",
      "Epoch: 935, Training loss: 24.55043837428093\n",
      "Epoch: 936, Training loss: 22.20448425412178\n",
      "Epoch: 937, Training loss: 24.94185644388199\n",
      "Epoch: 938, Training loss: 23.7555091381073\n",
      "Epoch: 939, Training loss: 24.253406822681427\n",
      "Epoch: 940, Training loss: 24.459606170654297\n",
      "Epoch: 941, Training loss: 23.35090982913971\n",
      "Epoch: 942, Training loss: 24.703457534313202\n",
      "Epoch: 943, Training loss: 25.31861314177513\n",
      "Epoch: 944, Training loss: 24.282048165798187\n",
      "Epoch: 945, Training loss: 24.710136353969574\n",
      "Epoch: 946, Training loss: 23.070280998945236\n",
      "Epoch: 947, Training loss: 24.869502186775208\n",
      "Epoch: 948, Training loss: 25.98221781849861\n",
      "Epoch: 949, Training loss: 24.32318538427353\n",
      "Epoch: 950, Training loss: 25.500860780477524\n",
      "Epoch: 951, Training loss: 25.138868987560272\n",
      "Epoch: 952, Training loss: 24.360006392002106\n",
      "Epoch: 953, Training loss: 24.199686110019684\n",
      "Epoch: 954, Training loss: 26.9716255068779\n",
      "Epoch: 955, Training loss: 26.330357491970062\n",
      "Epoch: 956, Training loss: 23.62574115395546\n",
      "Epoch: 957, Training loss: 23.526913940906525\n",
      "Epoch: 958, Training loss: 22.56413045525551\n",
      "Epoch: 959, Training loss: 26.171816527843475\n",
      "Epoch: 960, Training loss: 23.21049466729164\n",
      "Epoch: 961, Training loss: 26.341097444295883\n",
      "Epoch: 962, Training loss: 26.663113951683044\n",
      "Epoch: 963, Training loss: 24.206263095140457\n",
      "Epoch: 964, Training loss: 22.47527652978897\n",
      "Epoch: 965, Training loss: 23.75635552406311\n",
      "Epoch: 966, Training loss: 24.42379918694496\n",
      "Epoch: 967, Training loss: 25.21524840593338\n",
      "Epoch: 968, Training loss: 24.31320559978485\n",
      "Epoch: 969, Training loss: 24.743314534425735\n",
      "Epoch: 970, Training loss: 23.907717511057854\n",
      "Epoch: 971, Training loss: 24.67197075486183\n",
      "Epoch: 972, Training loss: 24.074642658233643\n",
      "Epoch: 973, Training loss: 25.797289967536926\n",
      "Epoch: 974, Training loss: 22.961557537317276\n",
      "Epoch: 975, Training loss: 24.223469018936157\n",
      "Epoch: 976, Training loss: 22.62062269449234\n",
      "Epoch: 977, Training loss: 25.025337785482407\n",
      "Epoch: 978, Training loss: 24.264835834503174\n",
      "Epoch: 979, Training loss: 23.158591628074646\n",
      "Epoch: 980, Training loss: 24.52868315577507\n",
      "Epoch: 981, Training loss: 24.15650787949562\n",
      "Epoch: 982, Training loss: 23.908579647541046\n",
      "Epoch: 983, Training loss: 25.679807007312775\n",
      "Epoch: 984, Training loss: 24.483458936214447\n",
      "Epoch: 985, Training loss: 24.73464146256447\n",
      "Epoch: 986, Training loss: 26.900758683681488\n",
      "Epoch: 987, Training loss: 23.927107274532318\n",
      "Epoch: 988, Training loss: 23.07679733633995\n",
      "Epoch: 989, Training loss: 24.125893503427505\n",
      "Epoch: 990, Training loss: 23.62526473402977\n",
      "Epoch: 991, Training loss: 22.63746678829193\n",
      "Epoch: 992, Training loss: 25.263535916805267\n",
      "Epoch: 993, Training loss: 23.39116409420967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 994, Training loss: 23.467604368925095\n",
      "Epoch: 995, Training loss: 25.1157568693161\n",
      "Epoch: 996, Training loss: 23.684837132692337\n",
      "Epoch: 997, Training loss: 24.958869755268097\n",
      "Epoch: 998, Training loss: 25.214031755924225\n",
      "Epoch: 999, Training loss: 22.72431269288063\n",
      "Epoch: 1000, Training loss: 25.685649514198303\n"
     ]
    }
   ],
   "source": [
    "fit(modelo,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(modelo, 'Clase15_w&b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
